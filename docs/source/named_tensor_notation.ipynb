{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Tensor Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**This is a translation of the [Named Tensor Notation (Chiang, Rush, Barak 2021)](https://namedtensor.github.io/) example from the [funsor](https://funsor.pyro.ai/) library to `effectful`. Much of the expository text is taken directly from the [original](https://github.com/pyro-ppl/funsor/blob/master/tutorials/named_tensor_notation_i.ipynb).**\n",
    "\n",
    "The mathematical notation with *named axes* introduced in [Named Tensor Notation (Chiang, Rush, Barak 2021)](https://namedtensor.github.io/) improves the readability of mathematical formulas involving multidimensional arrays. This includes tensor operations such as elementwise operations, reductions, contractions, renaming, indexing, and broadcasting. Part 1 covers examples from [2 Informal Overview](https://namedtensor.github.io/#sec:overview), [3.4.2 Advanced Indexing](https://namedtensor.github.io/#sec:examples), and [5 Formal Definitions](https://namedtensor.github.io/#sec:definitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Annotated\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "from effectful.ops.syntax import Bound, Scoped, defop\n",
    "from effectful.ops.semantics import evaluate, fvsof, handler\n",
    "from effectful.ops.types import Operation\n",
    "from effectful.handlers.torch import Indexable, sizesof, to_tensor\n",
    "\n",
    "\n",
    "def subst(term, substs):\n",
    "    with handler(\n",
    "        {k: functools.partial(lambda vv: vv, v) for (k, v) in substs.items()},\n",
    "    ):\n",
    "        return evaluate(term)\n",
    "\n",
    "\n",
    "def reduce(indexes, indexed_tensor, reducer):\n",
    "    \"\"\"Reduce an indexed tensor along one or more named dimensions.\n",
    "\n",
    "    Args:\n",
    "    - indexes: Names of dimensions to reduce.\n",
    "    - indexed_tensor: The tensor to reduce.\n",
    "    - reducer: A reduction function like `torch.sum`. Must take `tensor`, `dim`, and `keepdim` arguments.\n",
    "\n",
    "    Returns: A new indexed tensor with the specified dimensions reduced.\n",
    "\n",
    "    Example:\n",
    "    >>> width, height = defop(int, name='width'), defop(int, name='height')\n",
    "    >>> t = indexed(torch.ones(2, 3))[width(), height()]\n",
    "    >>> reduce([width], t, \"sum\")\n",
    "    indexed(tensor([2., 2., 2.]))[height()]\n",
    "    \"\"\"\n",
    "    fvars = fvsof(indexed_tensor)\n",
    "    indexes = [i for i in indexes if i in fvars]\n",
    "\n",
    "    # convert indexed dimensions to positional and flatten all new positional dims\n",
    "    t = to_tensor(indexed_tensor, indexes)\n",
    "    t_flat = torch.flatten(t, 0, len(indexes) - 1)\n",
    "\n",
    "    # reduce dim 0 into the first index of dim 0, then return reduction\n",
    "    return reducer(t_flat, 0, keepdim=True)[0]\n",
    "\n",
    "\n",
    "def gensyms(*names, type_=int):\n",
    "    return tuple(defop(int, name=n) for n in names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor axis is given a name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  A &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[3] \\times \\mathsf{\\vphantom{fg}width}[3]} = \\mathbb{R}^{\\mathsf{\\vphantom{fg}width}[3] \\times \\mathsf{\\vphantom{fg}height}[3]} \\\\\n",
    "  A &= \\mathsf{\\vphantom{fg}height}\n",
    "  \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "    3 & 1 & 4 \\\\\n",
    "    1 & 5 & 9 \\\\\n",
    "    2 & 6 & 5\n",
    "  \\end{bmatrix}\\end{array} =\n",
    "  \\mathsf{\\vphantom{fg}width}\n",
    "  \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}height}\\\\\\begin{bmatrix}\n",
    "    3 & 1 & 2 \\\\\n",
    "    1 & 5 & 6 \\\\\n",
    "    4 & 9 & 5\n",
    "  \\end{bmatrix}\\end{array}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[3, 1, 4],\n",
       "                  [1, 5, 9],\n",
       "                  [2, 6, 5]]))[height(), width()]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = defop(int, name=\"height\"), defop(int, name=\"width\")\n",
    "t = tensor([[3, 1, 4], [1, 5, 9], [2, 6, 5]])\n",
    "A = Indexable(tensor([[3, 1, 4], [1, 5, 9], [2, 6, 5]]))[height(), width()]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access elements of $A$ using named indices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{\\mathsf{\\vphantom{fg}height}(1), \\mathsf{\\vphantom{fg}width}(3)} = A_{\\mathsf{\\vphantom{fg}width}(3), \\mathsf{\\vphantom{fg}height}(1)} = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subst(A, {height: 0, width: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial indexing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A_{\\mathsf{\\vphantom{fg}height}(1)} &= \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\n",
    "\\begin{bmatrix}\n",
    "  3 & 1 & 4\n",
    "\\end{bmatrix}\\end{array}\n",
    "&\n",
    "A_{\\mathsf{\\vphantom{fg}width}(3)} &= \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}height}\\\\\n",
    "\\begin{bmatrix}\n",
    "  4 & 9 & 5\n",
    "\\end{bmatrix}\\end{array}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([3, 1, 4]))[width()]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subst(A, {height: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([4, 9, 5]))[height()]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subst(A, {width: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Operations and Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementwise operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac1{1+\\exp(-A)} = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\n",
    "\\begin{bmatrix}\n",
    "  \\frac 1{1+\\exp(-3)} & \\frac 1{1+\\exp(-1)} & \\frac 1{1+\\exp(-4)} \\\\[1ex]\n",
    "  \\frac 1{1+\\exp(-1)} & \\frac 1{1+\\exp(-5)} & \\frac 1{1+\\exp(-9)} \\\\[1ex]\n",
    "  \\frac 1{1+\\exp(-2)} & \\frac 1{1+\\exp(-6)} & \\frac 1{1+\\exp(-5)}\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[0.9526, 0.7311, 0.9820],\n",
       "                  [0.7311, 0.9933, 0.9999],\n",
       "                  [0.8808, 0.9975, 0.9933]]))[height(), width()]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (1 + (-A).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors with different shapes are automatically broadcasted against each other before an operation is applied. Let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  x &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[3]} & y &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}width}[3]} \\\\\n",
    "  x &= \\mathsf{\\vphantom{fg}height}\n",
    "  \\begin{array}[b]{@{}c@{}}\\\\\n",
    "  \\begin{bmatrix}\n",
    "    2 \\\\ 7 \\\\ 1\n",
    "  \\end{bmatrix}\\end{array} & \n",
    "  y &= \n",
    "  \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "    1 & 4 & 1\n",
    "  \\end{bmatrix}\\end{array}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Indexable(tensor([2, 7, 1]))[height()]\n",
    "y = Indexable(tensor([1, 4, 1]))[width()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary addition operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A + x &= \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "  3+2 & 1+2 & 4+2 \\\\\n",
    "  1+7 & 5+7 & 9+7 \\\\\n",
    "  2+1 & 6+1 & 5+1\n",
    "\\end{bmatrix}\\end{array} &\n",
    "A + y &= \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "  3+1 & 1+4 & 4+1 \\\\\n",
    "  1+1 & 5+4 & 9+1 \\\\\n",
    "  2+1 & 6+4 & 5+1\n",
    "\\end{bmatrix}\\end{array}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 5,  3,  6],\n",
       "                  [ 8, 12, 16],\n",
       "                  [ 3,  7,  6]]))[height(), width()]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 4,  5,  5],\n",
       "                  [ 2,  9, 10],\n",
       "                  [ 3, 10,  6]]))[height(), width()]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary multiplication operation:\n",
    "\n",
    "$$\n",
    "A \\odot x = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "  3\\cdot2 & 1\\cdot2 & 4\\cdot2 \\\\\n",
    "  1\\cdot7 & 5\\cdot7 & 9\\cdot7 \\\\\n",
    "  2\\cdot1 & 6\\cdot1 & 5\\cdot1\n",
    "\\end{bmatrix}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 6,  2,  8],\n",
       "                  [ 7, 35, 63],\n",
       "                  [ 2,  6,  5]]))[height(), width()]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary maximum operation:\n",
    "\n",
    "$$\n",
    "\\max(A, y) = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\\begin{bmatrix}\n",
    "  \\max(3, 1) & \\max(1, 4) & \\max(4, 1) \\\\\n",
    "  \\max(1, 1) & \\max(5, 4) & \\max(9, 1) \\\\\n",
    "  \\max(2, 1) & \\max(6, 4) & \\max(5, 1)\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[3, 4, 4],\n",
       "                  [1, 5, 9],\n",
       "                  [2, 6, 5]]))[height(), width()]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named axes can be reduced over by calling the `.reduce` method and specifying the [reduction operator](https://en.wikipedia.org/wiki/Reduction_Operator) and names of reduced axes. Note that reduction is defined only for operators that are associative and commutative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum\\limits_{\\substack{\\mathsf{\\vphantom{fg}height}}} A = \\sum_i A_{\\mathsf{\\vphantom{fg}height}(i)} = \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\n",
    "\\begin{bmatrix}\n",
    "  3+1+2 & 1+5+6 & 4+9+5\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([ 6, 12, 18]))[width()]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height], A, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum\\limits_{\\substack{\\mathsf{\\vphantom{fg}width}}} A = \\sum_j A_{\\mathsf{\\vphantom{fg}width}(j)} = \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}height}\\\\\n",
    "\\begin{bmatrix}\n",
    "  3+1+4 & 1+5+9 & 2+6+5\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([ 8, 15, 13]))[height()]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([width], A, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction over multiple axes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum\\limits_{\\substack{\\mathsf{\\vphantom{fg}height}\\\\\n",
    " \\mathsf{\\vphantom{fg}width}}} A = \\sum_i \\sum_j A_{\\mathsf{\\vphantom{fg}height}(i),\\mathsf{\\vphantom{fg}width}(j)} = 3+1+4+1+5+9+2+6+5.\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height, width], A, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\prod\\limits_{\\substack{\\mathsf{\\vphantom{fg}height}}} A = \\prod_i A_{\\mathsf{\\vphantom{fg}height}(i)} = \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\n",
    "\\begin{bmatrix}\n",
    "  3\\cdot1\\cdot2 & 1\\cdot5\\cdot6 & 4\\cdot9\\cdot5\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([  6,  30, 180]))[width()]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height], A, torch.prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max\\limits_{\\substack{\\mathsf{\\vphantom{fg}height}}} A = \\max \\{A_{\\mathsf{\\vphantom{fg}height}(i)} \\mid 1 \\leq i \\leq n\\} = \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\\\\\n",
    "\\begin{bmatrix}\n",
    "  \\max(3, 1, 2) & \\max(1, 5, 6) & \\max(4, 9, 5)\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([3, 6, 9]))[width()]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height], A, torch.amax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction operation can be written as elementwise multiplication followed by summation over an axis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}width}}}{\\vphantom{fg}\\odot}} y = \\sum_j A_{\\mathsf{\\vphantom{fg}width}(j)} \\, y_{\\mathsf{\\vphantom{fg}width}(j)} = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\\\\\begin{bmatrix}\n",
    "  3\\cdot 1 + 1\\cdot 4 + 4\\cdot 1 \\\\\n",
    "  1\\cdot 1 + 5\\cdot 4 + 9\\cdot 1 \\\\\n",
    "  2\\cdot 1 + 6\\cdot 4 + 5\\cdot 1\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([11, 30, 31]))[height()]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([width], A * y, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other operations from linear algebra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}}}{\\vphantom{fg}\\odot}} x = \\sum_i x_{\\mathsf{\\vphantom{fg}height}(i)} \\, x_{\\mathsf{\\vphantom{fg}height}(i)} \\qquad \\text{inner product}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height], x * x, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "[x \\odot y]_{\\mathsf{\\vphantom{fg}height}(i), \\mathsf{\\vphantom{fg}width}(j)} = x_{\\mathsf{\\vphantom{fg}height}(i)} \\, y_{\\mathsf{\\vphantom{fg}width}(j)} \\qquad \\text{outer product}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 2,  8,  2],\n",
       "                  [ 7, 28,  7],\n",
       "                  [ 1,  4,  1]]))[height(), width()]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}width}}}{\\vphantom{fg}\\odot}} y = \\sum_i A_{\\mathsf{\\vphantom{fg}width}(i)} \\, y_{\\mathsf{\\vphantom{fg}width}(i)} \\qquad \\text{matrix-vector product}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([11, 30, 31]))[height()]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([width], A * y, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}}}{\\vphantom{fg}\\odot}} A = \\sum_i x_{\\mathsf{\\vphantom{fg}height}(i)} \\, A_{\\mathsf{\\vphantom{fg}height}(i)} \\qquad \\text{vector-matrix product} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([15, 43, 76]))[width()]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([height], x * A, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}width}}}{\\vphantom{fg}\\odot}} B = \\sum_i A_{\\mathsf{\\vphantom{fg}width}(i)} \\odot B_{\\mathsf{\\vphantom{fg}width}(i)} \\qquad \\text{matrix-matrix product}~(B \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}width}\\times \\mathsf{\\vphantom{fg}width2}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 46,  22,  39],\n",
       "                  [100,  49,  59],\n",
       "                  [ 76,  43,  40]]))[height(), width2()]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width2 = defop(int, name=\"width2\")\n",
    "B = Indexable(\n",
    "    tensor([[3, 2, 5], [5, 4, 0], [8, 3, 6]]),\n",
    ")[width(), width2()]\n",
    "\n",
    "reduce([width], A * B, torch.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction can be generalized to other binary and reduction operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_{\\mathsf{\\vphantom{fg}width}} (A + y) = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\\\\\begin{bmatrix}\n",
    "  \\max(3+1, 1+4, 4+1) \\\\\n",
    "  \\max(1+1, 5+4, 9+1) \\\\\n",
    "  \\max(2+1, 6+4, 5+1)\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([ 5, 10, 10]))[height()]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce([width], A + y, torch.amax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming named dimensions is simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{\\mathsf{\\vphantom{fg}height}\\rightarrow\\mathsf{\\vphantom{fg}height2}} = \\mathsf{\\vphantom{fg}height2}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\n",
    "\\\\\\begin{bmatrix}\n",
    "  3 & 1 & 4 \\\\\n",
    "  1 & 5 & 9 \\\\\n",
    "  2 & 6 & 5 \\\\\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[3, 1, 4],\n",
       "                  [1, 5, 9],\n",
       "                  [2, 6, 5]]))[height2(), width()]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height2 = defop(int, name=\"height2\")\n",
    "subst(A, {height: height2()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{(\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}width})\\rightarrow\\mathsf{\\vphantom{fg}layer}} = \\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}layer}\\\\\n",
    "\\begin{bmatrix}\n",
    "    3 & 1 & 4 & 1 & 5 & 9 & 2 & 6 & 5\n",
    "\\end{bmatrix}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "layer = defop(int, name=\"layer\")\n",
    "A_layer = subst(A, {height: layer() // 3, width: layer() % 3})\n",
    "print(subst(A_layer, {layer: 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{\\mathsf{\\vphantom{fg}layer}\\rightarrow(\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}width})} = \\mathsf{\\vphantom{fg}height}\n",
    "\\begin{array}[b]{@{}c@{}}\\mathsf{\\vphantom{fg}width}\n",
    "\\\\\\begin{bmatrix}\n",
    "  3 & 1 & 4 \\\\\n",
    "  1 & 5 & 9 \\\\\n",
    "  2 & 6 & 5 \\\\\n",
    "\\end{bmatrix}\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_torch_op(tensor([[3, 1, 4],\n",
      "        [1, 5, 9],\n",
      "        [2, 6, 5]]), ['floordiv(\\'add(\"mul(\\\\\\'height()\\\\\\', 3, )\", \"mod(\\\\\\'width()\\\\\\', 3, )\", )\\', 3, )', 'mod(\\'add(\"mul(\\\\\\'height()\\\\\\', 3, )\", \"mod(\\\\\\'width()\\\\\\', 3, )\", )\\', 3, )'], )\n"
     ]
    }
   ],
   "source": [
    "print(subst(A_layer, {layer: height() * 3 + width() % 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of advanced indexing can be achieved through name substitutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{index}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}ax}[n]} \\times [n] \\rightarrow \\mathbb{R}\\\\\n",
    "\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{index}}}(A, i) = A_{\\mathsf{\\vphantom{fg}ax}(i)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  E &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}vocab}[n] \\times \\mathsf{\\vphantom{fg}emb}} \\\\\n",
    "  i &\\in [n] \\\\\n",
    "  I &\\in [n]^{\\mathsf{\\vphantom{fg}seq}} \\\\\n",
    "  P &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}vocab}[n]}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial indexing $\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{index}}}(E,i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([1, 3, 7]))[emb()]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, emb = defop(int, name=\"vocab\"), defop(int, name=\"emb\")\n",
    "E = Indexable(\n",
    "    tensor([[2, 1, 5], [3, 4, 2], [1, 3, 7], [1, 4, 3], [5, 9, 2]]),\n",
    ")[vocab(), emb()]\n",
    "\n",
    "subst(E, {vocab: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer array indexing $\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{index}}}(E,I)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[1, 4, 3],\n",
       "                  [1, 3, 7],\n",
       "                  [5, 9, 2],\n",
       "                  [2, 1, 5]]))[seq(), emb()]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = defop(int, name=\"seq\")\n",
    "I = Indexable(tensor([3, 2, 4, 0]))[seq()]\n",
    "\n",
    "subst(E, {vocab: I})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather operation $\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{index}}}(P,I)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([1, 5, 2, 2]))[seq()]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = Indexable(\n",
    "    tensor([[6, 2, 4, 2], [8, 2, 1, 3], [5, 5, 7, 0], [1, 3, 8, 2], [5, 9, 2, 3]]),\n",
    ")[vocab(), seq()]\n",
    "\n",
    "subst(P, {vocab: I})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with two integer arrays:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  |\\mathsf{\\vphantom{fg}seq}| &= m \\\\\n",
    "  I_1 &= [m]^\\mathsf{\\vphantom{fg}subseq}\\\\\n",
    "  I_2 &= [n]^\\mathsf{\\vphantom{fg}subseq}\\\\\n",
    "  S &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{index}}}(\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\mathrm{index}}}(P, I_1), I_2) \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}subseq}} \\\\\n",
    "  S_{\\mathsf{\\vphantom{fg}subseq}(i)} &= P_{\\mathsf{\\vphantom{fg}seq}(I_{\\mathsf{\\vphantom{fg}subseq}(i)}), \\mathsf{\\vphantom{fg}vocab}(I_{\\mathsf{\\vphantom{fg}subseq}(i)})}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([3, 4, 5]))[subseq()]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subseq = defop(int, name=\"subseq\")\n",
    "I1 = Indexable(tensor([1, 2, 0]))[subseq()]\n",
    "I2 = Indexable(tensor([3, 0, 4]))[subseq()]\n",
    "\n",
    "subst(P, {seq: I1, vocab: I2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} \\\\\n",
    "  X^1 &= \\sigma(W^1 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} X^0 + b^1) & W^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1 \\times \\mathsf{\\vphantom{fg}input}} & b^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1} \\\\\n",
    "  X^2 &= \\sigma(W^2 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_1}}{\\vphantom{fg}\\odot}} X^1 + b^2) & W^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2 \\times \\mathsf{\\vphantom{fg}hidden}_1} & b^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2} \\\\\n",
    "  X^3 &= \\sigma(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_2}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}\\times \\mathsf{\\vphantom{fg}hidden}_2} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}[n_0]} \\\\\n",
    "W^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer^2}[n_l] \\times \\mathsf{\\vphantom{fg}layer}[n_{l-1}]} \\\\\n",
    "  b^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer^2}[n_l]} \\\\\n",
    "  \\text{FullConn}^l(x) &= \\sigma\\left(W^l \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} x + b^l\\right)_{\\mathsf{\\vphantom{fg}layer^2}\\rightarrow\\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def FullConn(\n",
    "    x: torch.Tensor,\n",
    "    W: torch.Tensor,\n",
    "    b: torch.Tensor,\n",
    "    layer: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    return reduce([layer], torch.sigmoid(torch.mul(W, x)), torch.sum) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([50.4928, 50.3666, 46.5755, 51.0902, 48.0302, 47.7592, 53.8888, 49.4391,\n",
       "                  46.0442, 50.7626, 52.0595, 49.6586, 49.4210, 51.5147, 51.5289, 52.1901,\n",
       "                  49.6542, 51.2773, 51.1176, 52.5399, 51.3027, 50.5177, 47.7546, 48.5222,\n",
       "                  48.2768, 49.6295, 47.6673, 48.1795, 52.2634, 51.3343, 52.4435, 51.9259]))[output()]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 100\n",
    "output_size = 32\n",
    "input_, output = defop(int, name=\"input\"), defop(int, name=\"output\")\n",
    "\n",
    "W = Indexable(torch.randn(input_size, output_size))[input_(), output()]\n",
    "b = Indexable(torch.randn(output_size))[output()]\n",
    "X = Indexable(torch.randn(input_size))[input_()]\n",
    "\n",
    "FullConn(X, W, b, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x^{t} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} & t &= 1, \\ldots, n \\\\\n",
    "W^{\\text{h}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} & |\\mathsf{\\vphantom{fg}hidden}| &= |\\mathsf{\\vphantom{fg}hidden}^\\prime| \\\\\n",
    "W^{\\text{i}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "b &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "h^{0} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "h^{t} &= \\sigma\\left( W^{\\text{h}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} h^{t-1} + W^{\\text{i}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} x^{t} + b \\right)_{\\mathsf{\\vphantom{fg}hidden}^\\prime\\rightarrow\\mathsf{\\vphantom{fg}hidden}} & t &= 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def RNN(\n",
    "    x: torch.Tensor,\n",
    "    Wh: torch.Tensor,\n",
    "    Wi: torch.Tensor,\n",
    "    b: torch.Tensor,\n",
    "    h: torch.Tensor,\n",
    "    hidden: Annotated[Operation[[], int], Bound()],\n",
    "    layer: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    return torch.sigmoid(\n",
    "        reduce([hidden], Wh * h, torch.sum) + reduce([layer], Wi * x, torch.sum) + b\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([5.3164e-04, 9.9644e-01, 5.8283e-03, 1.1841e-03, 9.7848e-01, 7.4228e-01,\n",
       "                  1.0409e-02, 7.9958e-01, 9.9357e-01, 5.5008e-01, 4.7147e-07, 9.9994e-01,\n",
       "                  1.3836e-02, 6.3806e-02, 9.9164e-01, 6.2719e-01, 1.5871e-14, 4.1970e-04,\n",
       "                  3.2708e-05, 1.0000e+00, 1.0000e+00, 8.6289e-04, 5.2287e-02, 9.9997e-01,\n",
       "                  2.0621e-10, 7.4793e-01, 8.1911e-01, 1.1897e-10, 9.9997e-01, 1.1194e-02,\n",
       "                  1.6546e-06, 9.3164e-01]))[hidden2()]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 100\n",
    "hidden_size = 32\n",
    "input_, hidden, hidden2 = gensyms(\"input\", \"hidden\", \"hidden2\")\n",
    "\n",
    "Wh = Indexable(torch.randn(hidden_size, hidden_size))[hidden(), hidden2()]\n",
    "Wi = Indexable(torch.randn(input_size, hidden_size))[input_(), hidden2()]\n",
    "b = Indexable(torch.randn(hidden_size))[hidden2()]\n",
    "h = Indexable(torch.randn(hidden_size))[hidden()]\n",
    "x = Indexable(torch.randn(input_size))[input_()]\n",
    "\n",
    "RNN(x, Wh, Wi, b, h, hidden, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Softmax(\n",
    "    x: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()],\n",
    "    ax2: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    x = subst(x, {ax: ax2()})\n",
    "    y = x - reduce([ax2], x, torch.logsumexp)\n",
    "    return y.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\text{Attention} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}val}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}val}} \\\\\n",
    "\\text{Attention}(Q, K, V, M) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\mathrm{softmax}}} \\left( \\frac{Q \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}key}}}{\\vphantom{fg}\\odot}} K}{\\sqrt{|\\mathsf{\\vphantom{fg}key}|}} + M \\right) \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\odot}} V.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    M: torch.Tensor,\n",
    "    key: Annotated[Operation[[], int], Bound()],\n",
    "    seq: Annotated[Operation[[], int], Bound()],\n",
    "    seq2: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    x = reduce([key], Q * K, torch.sum) / sizesof(Q)[key] + M\n",
    "    return reduce([seq], Softmax(x, seq, seq2) * V, torch.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 0.5492, -0.1205,  0.7084,  1.2042,  0.1465],\n",
       "                  [ 0.4034, -0.0885,  0.5204,  0.8846,  0.1076],\n",
       "                  [ 0.5791, -0.1271,  0.7470,  1.2698,  0.1544]]))[seq2(), val()]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_size = 10\n",
    "val_size = 5\n",
    "seq_size = 3\n",
    "\n",
    "key, val, seq, seq2 = gensyms(\"key\", \"val\", \"seq\", \"seq2\")\n",
    "Q = Indexable(torch.randn(key_size))[key()]\n",
    "K = Indexable(torch.randn(key_size, seq_size))[key(), seq()]\n",
    "V = Indexable(torch.randn(seq_size, val_size))[seq(), val()]\n",
    "M = Indexable(torch.randn(seq_size))[seq()]\n",
    "\n",
    "Attention(Q, K, V, M, key, seq, seq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n-|\\mathsf{\\vphantom{fg}kernel}|+1], \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}(i+j - 1)}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Unroll(\n",
    "    x: Annotated[torch.Tensor, Scoped(0)],\n",
    "    seq: Annotated[Operation[[], int], Bound(0)],\n",
    "    k: int,\n",
    "    kernel: Annotated[Operation[[], int], Scoped(1)],\n",
    "    seq2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    return Indexable(to_tensor(x, [seq]).unfold(0, k, 1))[seq2(), kernel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{Conv1d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n^\\prime]} \\\\\n",
    "\\text{Conv1d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "b &\\in \\mathbb{R}\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Conv1d(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    W: Annotated[torch.Tensor, Scoped(1)],\n",
    "    b: Annotated[torch.Tensor, Scoped(2)],\n",
    "    chans: Annotated[Operation[[], int], Bound(1)],\n",
    "    k: int,\n",
    "    kernel: Annotated[Operation[[], int], Bound(1)],\n",
    "    seq: Annotated[Operation[[], int], Bound(0)],\n",
    "    seq2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    y = W * Unroll(X, seq, k, kernel, seq2)\n",
    "    return reduce([chans, kernel], y, torch.sum) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([-2.6634, -1.5362, -3.1277, -4.3292, -5.1438,  0.0169,  2.1813,  4.3929]))[seq2()]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chans_size = 3\n",
    "seq_size = 10\n",
    "kernel_size = 3\n",
    "\n",
    "chans, kernel, seq, seq2 = gensyms(\"chans\", \"kernel\", \"seq\", \"seq2\")\n",
    "\n",
    "X = Indexable(torch.randn(chans_size, seq_size))[chans(), seq()]\n",
    "W = Indexable(torch.randn(chans_size, kernel_size))[chans(), kernel()]\n",
    "b = torch.randn(tuple())\n",
    "\n",
    "Conv1d(X, W, b, chans, 3, kernel, seq, seq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{Conv2d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}height}[h] \\times \\mathsf{\\vphantom{fg}width}[w]}\n",
    "  &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h2] \\times \\mathsf{\\vphantom{fg}width}[w2]} \\\\\n",
    "  \\text{Conv2d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kh}, \\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}\\\\ \\mathsf{\\vphantom{fg}kh}}}{\\vphantom{fg}\\mathrm{unroll}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}width}\\\\\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kh}\\times \\mathsf{\\vphantom{fg}kw}} \\\\\n",
    "b &\\in \\mathbb{R}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Conv2d(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    W: Annotated[torch.Tensor, Scoped(1)],\n",
    "    b: Annotated[torch.Tensor, Scoped(2)],\n",
    "    chans: Annotated[Operation[[], int], Bound(1)],\n",
    "    kh_size: int,\n",
    "    kh: Annotated[Operation[[], int], Bound(1)],\n",
    "    height: Annotated[Operation[[], int], Bound(0)],\n",
    "    height2: Annotated[Operation[[], int], Scoped(1)],\n",
    "    kw_size: int,\n",
    "    kw: Annotated[Operation[[], int], Bound(1)],\n",
    "    width: Annotated[Operation[[], int], Bound(0)],\n",
    "    width2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    y = W * Unroll(Unroll(X, width, kw_size, kw, width2), height, kh_size, kh, height2)\n",
    "    return reduce([chans, kh, kw], y, torch.sum) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[  7.6198,  13.8169,  -9.2522,  -7.3932,  -5.4929,  -4.6409,   1.0257,\n",
       "                     1.3990],\n",
       "                  [-10.1009,   5.4362,   6.9355,  -2.4888,   6.2462,  -5.4670,   1.6333,\n",
       "                    -1.7763],\n",
       "                  [ -0.4922,   2.3385,  -7.0648,   7.1834,   6.3221,   2.8002,   1.1057,\n",
       "                     4.0543],\n",
       "                  [ -9.9722,   4.4106,  -1.2959,  -3.7148,   1.7388,   4.9986,   2.5309,\n",
       "                    -5.0319],\n",
       "                  [  4.1358,   0.5310,  -6.6142,  -5.4471, -11.8705,   8.4479,   0.0536,\n",
       "                   -11.9730]]))[width2(), height2()]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chans_size = 3\n",
    "kh_size = 3\n",
    "kw_size = 4\n",
    "height_size = 10\n",
    "width_size = 8\n",
    "\n",
    "chans, kh, kw, height, width, height2, width2 = gensyms(\n",
    "    \"chans\", \"kh\", \"kw\", \"height\", \"width\", \"height2\", \"width2\"\n",
    ")\n",
    "\n",
    "X = Indexable(torch.randn(chans_size, height_size, width_size))[\n",
    "    chans(), height(), width()\n",
    "]\n",
    "W = Indexable(torch.randn(chans_size, kh_size, kw_size))[chans(), kh(), kw()]\n",
    "b = torch.randn(tuple())\n",
    "\n",
    "Conv2d(X, W, b, chans, kh_size, kh, height, height2, kw_size, kw, width, width2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n/|\\mathsf{\\vphantom{fg}kernel}|],\\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}((i-1) \\cdot |\\mathsf{\\vphantom{fg}kernel}| + j)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Pool(\n",
    "    x: Annotated[torch.Tensor, Scoped(0)],\n",
    "    seq: Annotated[Operation[[], int], Bound(0)],\n",
    "    k: int,\n",
    "    kernel: Annotated[Operation[[], int], Scoped(1)],\n",
    "    seq2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    xp = to_tensor(x, [seq])\n",
    "    return Indexable(xp.reshape((xp.shape[0] // k, k) + xp.shape[1:]))[seq2(), kernel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 1.1593,  0.7818],\n",
       "                  [-1.3174,  0.4712],\n",
       "                  [-1.0878,  1.6868],\n",
       "                  [-0.2595, -2.7358],\n",
       "                  [-1.0627, -1.2987]]))[seq2(), kernel()]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 10\n",
    "seq, seq2, kernel = gensyms(\"seq\", \"seq2\", \"kernel\")\n",
    "\n",
    "X = Indexable(torch.randn(seq_size))[seq()]\n",
    "Y = Pool(X, seq, 2, kernel, seq2)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MaxPool1d}_{k} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n/k]} \\\\\n",
    "\\text{MaxPool1d}_{k}(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{max}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X \\\\\n",
    "|\\mathsf{\\vphantom{fg}kernel}| &= k \\\\\n",
    "\\text{MaxPool2d}_{kh,kw} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h] \\times \\mathsf{\\vphantom{fg}width}[w]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h/kh] \\times \\mathsf{\\vphantom{fg}width}[w/kw]} \\\\\n",
    "\\text{MaxPool2d}_{kh,kw}(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kh},\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{max}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}kh}}}{\\vphantom{fg}\\mathrm{pool}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}width},\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{pool}}} X \\\\\n",
    "|\\mathsf{\\vphantom{fg}kh}| &= kh \\\\\n",
    "|\\mathsf{\\vphantom{fg}kw}| &= kw.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def MaxPool1d(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    seq: Annotated[Operation[[], int], Bound(0)],\n",
    "    k: int,\n",
    "    kernel: Annotated[Operation[[], int], Scoped(1)],\n",
    "    seq2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    return reduce([kernel], Pool(X, seq, k, kernel, seq2), torch.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[ 1.7442],\n",
       "                  [ 2.5824],\n",
       "                  [-0.3881],\n",
       "                  [ 1.2570],\n",
       "                  [ 0.9666]]))[seq2(), slice(None, None, None)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 10\n",
    "\n",
    "seq, seq2, kernel = gensyms(\"seq\", \"seq2\", \"kernel\")\n",
    "\n",
    "X = Indexable(torch.randn(seq_size))[seq()]\n",
    "MaxPool1d(X, seq, 2, kernel, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def MaxPool2d(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    height: Annotated[Operation[[], int], Bound(0)],\n",
    "    kh_size: int,\n",
    "    kh: Annotated[Operation[[], int], Bound(1)],\n",
    "    height2: Annotated[Operation[[], int], Scoped(1)],\n",
    "    width: Annotated[Operation[[], int], Bound(0)],\n",
    "    kw_size: int,\n",
    "    kw: Annotated[Operation[[], int], Bound(1)],\n",
    "    width2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    y = Pool(Pool(X, height, kh_size, kh, height2), width, kw_size, kw, width2)\n",
    "    return reduce([kh, kw], y, torch.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[[0.6379],\n",
       "                   [1.4800],\n",
       "                   [2.2258]],\n",
       "          \n",
       "                  [[1.0709],\n",
       "                   [1.0846],\n",
       "                   [1.7210]]]))[height2(), width2(), slice(None, None, None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width_size = 9\n",
    "height_size = 4\n",
    "\n",
    "width, width2, height, height2, kw, kh = gensyms(\n",
    "    \"width\", \"width2\", \"height\", \"height2\", \"kw\", \"kh\"\n",
    ")\n",
    "\n",
    "X = Indexable(torch.randn(width_size, height_size))[width(), height()]\n",
    "MaxPool2d(X, height, 2, kh, height2, width, 3, kw, width2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{standardize}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}ax}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}ax}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) &= \\frac{X - \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{mean}}}(X)}{\\sqrt{\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{var}}}(X) + \\epsilon}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Mean(\n",
    "    X: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()]\n",
    ") -> torch.Tensor:\n",
    "    return reduce([ax], X, torch.sum) / sizesof(X)[ax]\n",
    "\n",
    "\n",
    "@defop\n",
    "def Mean2(\n",
    "    X: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()],\n",
    "    ax2: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    sizes = sizesof(X)\n",
    "    return reduce([ax, ax2], X, torch.sum) / (sizes[ax] * sizes[ax2])\n",
    "\n",
    "\n",
    "@defop\n",
    "def Variance(\n",
    "    X: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()]\n",
    ") -> torch.Tensor:\n",
    "    return Mean((X - Mean(X, ax)) ** 2, ax)\n",
    "\n",
    "\n",
    "@defop\n",
    "def Variance2(\n",
    "    X: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()],\n",
    "    ax2: Annotated[Operation[[], int], Bound()],\n",
    ") -> torch.Tensor:\n",
    "    return Mean2((X - Mean2(X, ax, ax2)) ** 2, ax, ax2)\n",
    "\n",
    "\n",
    "@defop\n",
    "def Standardize(\n",
    "    X: torch.Tensor,\n",
    "    ax: Annotated[Operation[[], int], Bound()],\n",
    "    new_ax: Operation[[], int],\n",
    ") -> torch.Tensor:\n",
    "    y = subst(X, {ax: new_ax()})\n",
    "    return (y - Mean(X, ax)) / (Variance(X, ax) + torch.finfo(X.dtype).eps).sqrt()\n",
    "\n",
    "\n",
    "@defop\n",
    "def Standardize2(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    ax: Annotated[Operation[[], int], Bound(0)],\n",
    "    ax2: Annotated[Operation[[], int], Bound(0)],\n",
    "    new_ax: Annotated[Operation[[], int], Scoped(1)],\n",
    "    new_ax2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    y = subst(X, {ax: new_ax(), ax2: new_ax2()})\n",
    "    return (y - Mean2(X, ax, ax2)) / (\n",
    "        Variance2(X, ax, ax2) + torch.finfo(X.dtype).eps\n",
    "    ).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{BatchNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}batch},\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}} \\\\\n",
    "\\text{InstanceNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}} \\\\\n",
    "\\text{LayerNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer},\\mathsf{\\vphantom{fg}chans}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans},\\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def BatchNorm(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    gamma: Annotated[torch.Tensor, Scoped(1)],\n",
    "    beta: Annotated[torch.Tensor, Scoped(1)],\n",
    "    batch: Annotated[Operation[[], int], Bound(0)],\n",
    "    layer: Annotated[Operation[[], int], Bound(0)],\n",
    "    batch2: Annotated[Operation[[], int], Scoped(1)],\n",
    "    layer2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    return Standardize2(X, batch, layer, batch2, layer2) * gamma + beta\n",
    "\n",
    "\n",
    "@defop\n",
    "def InstanceNorm(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    gamma: Annotated[torch.Tensor, Scoped(1)],\n",
    "    beta: Annotated[torch.Tensor, Scoped(1)],\n",
    "    layer: Annotated[Operation[[], int], Bound(0)],\n",
    "    layer2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    return Standardize(X, layer, layer2) * gamma + beta\n",
    "\n",
    "\n",
    "# same as BatchNorm\n",
    "@defop\n",
    "def LayerNorm(\n",
    "    X: Annotated[torch.Tensor, Scoped(0)],\n",
    "    gamma: Annotated[torch.Tensor, Scoped(1)],\n",
    "    beta: Annotated[torch.Tensor, Scoped(1)],\n",
    "    chans: Annotated[Operation[[], int], Bound(0)],\n",
    "    layer: Annotated[Operation[[], int], Bound(0)],\n",
    "    chans2: Annotated[Operation[[], int], Scoped(1)],\n",
    "    layer2: Annotated[Operation[[], int], Scoped(1)],\n",
    ") -> torch.Tensor:\n",
    "    return Standardize2(X, chans, layer, chans2, layer2) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[[-0.3860, -0.0208,  1.6471,  1.1876,  0.7335],\n",
       "                   [-0.5364, -0.5544, -0.6808, -0.7777, -0.6160],\n",
       "                   [-2.6801,  0.7606, -0.3962, -0.4761, -0.1108]],\n",
       "          \n",
       "                  [[-0.8772,  2.8144,  1.0492,  0.3852, -1.6412],\n",
       "                   [-0.7347, -0.7015, -0.4701, -0.4021, -0.2201],\n",
       "                   [ 0.8150,  2.7556, -3.6784, -2.2765, -1.2209]],\n",
       "          \n",
       "                  [[-0.3100,  1.1549,  1.5129,  0.3338, -0.1662],\n",
       "                   [-0.5372, -0.5734, -0.4665, -0.7560, -0.3452],\n",
       "                   [-0.7884,  0.5170, -0.4769, -0.3939, -0.3667]],\n",
       "          \n",
       "                  [[ 2.5132,  3.0934,  0.4354,  1.2032,  1.3621],\n",
       "                   [-0.4638, -0.3542, -0.3342, -0.5386, -0.4913],\n",
       "                   [-0.7143,  0.7918, -0.9467,  0.4952,  1.1523]]]))[batch2(), chans(), layer2()]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, chans_size, layer_size = 4, 3, 5\n",
    "batch, batch2, chans, layer, layer2 = gensyms(\n",
    "    \"batch\", \"batch2\", \"chans\", \"layer\", \"layer2\"\n",
    ")\n",
    "\n",
    "x = Indexable(torch.randn(batch_size, chans_size, layer_size))[\n",
    "    batch(), chans(), layer()\n",
    "]\n",
    "g = Indexable(torch.randn(chans_size))[chans()]\n",
    "b = Indexable(torch.randn(chans_size))[chans()]\n",
    "\n",
    "BatchNorm(x, g, b, batch, layer, batch2, layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{GroupNorm}_k(X; \\gamma, \\beta) &= \\left[ \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kernel},\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}, \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X \\right]_{(\\mathsf{\\vphantom{fg}chans},\\mathsf{\\vphantom{fg}kernel})\\rightarrow \\mathsf{\\vphantom{fg}chans}} \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "|\\mathsf{\\vphantom{fg}kernel}| &= k\\\\\n",
    "\\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  I &\\in \\{0, 1\\}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}vocab}} & \\sum\\limits_{\\substack{\\mathsf{\\vphantom{fg}vocab}}} I &= 1 \\\\\n",
    "  W &= (E \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\odot}} I)\\sqrt{|\\mathsf{\\vphantom{fg}layer}|} & E &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}vocab}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  P &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  P_{\\mathsf{\\vphantom{fg}seq}(p), \\mathsf{\\vphantom{fg}layer}(i)} &= \\begin{cases}\n",
    "    \\sin((p-1) / 10000^{(i-1) / |\\mathsf{\\vphantom{fg}layer}|}) & \\text{$i$ odd} \\\\ \n",
    "    \\cos((p-1) / 10000^{(i-2) / |\\mathsf{\\vphantom{fg}layer}|}) & \\text{$i$ even.}\n",
    "  \\end{cases}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^0 &= W+P \\\\\n",
    "T^1 &= \\text{LayerNorm}^1(\\text{SelfAtt}^1(X^0)) + X^0\\\\\n",
    "X^1 &= \\text{LayerNorm}^{1^\\prime}(\\text{FFN}^1(T^1)) + T^1\\\\\n",
    "&\\vdotswithin{=} \\\\\n",
    "T^{L} &= \\text{LayerNorm}^L(\\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\\\\n",
    "X^{L} &= \\text{LayerNorm}^{L^\\prime}(\\text{FFN}^L(T^L)) + T^L\\\\\n",
    "O &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{softmax}}}(E \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X^L)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{LayerNorm}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{LayerNorm}^l(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{XNorm}}}(X; \\beta^l, \\gamma^l).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{SelfAtt}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{SelfAtt}^l(X) &= Y\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  |\\mathsf{\\vphantom{fg}seq}| &= |\\mathsf{\\vphantom{fg}seq2}| \\\\\n",
    "  |\\mathsf{\\vphantom{fg}key}| = |\\mathsf{\\vphantom{fg}val}| &= |\\mathsf{\\vphantom{fg}layer}|/|\\mathsf{\\vphantom{fg}heads}| \\\\\n",
    "  Q &= W^{l,Q} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X_{\\mathsf{\\vphantom{fg}seq}\\rightarrow\\mathsf{\\vphantom{fg}seq2}} & W^{l,Q} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}key}} \\\\\n",
    "  K &= W^{l,K} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X & W^{l,K} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}key}} \\\\\n",
    "  V &= W^{l,V} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X & W^{l,V} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}val}} \\\\\n",
    "  M & \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}seq2}} \\\\\n",
    "  M_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}seq2}(j)} &= \\begin{cases}\n",
    "    0 & i \\leq j\\\\\n",
    "    -\\infty & \\text{otherwise}\n",
    "  \\end{cases} \\\\\n",
    "  Y &= W^{l,O} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}heads}\\\\ \\mathsf{\\vphantom{fg}val}}}{\\vphantom{fg}\\odot}} \\text{Attention}(Q, K, V, M)_{\\mathsf{\\vphantom{fg}seq2}\\rightarrow\\mathsf{\\vphantom{fg}seq}} & W^{l,O} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}val}\\times \\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{FFN}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{FFN}^l(X) &= X^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  X^1 &= \\text{relu}(W^{l,1} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X + b^{l,1}) & W^{l,1} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}layer}} & b^{l,1} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "  X^2 &= \\text{relu}(W^{l,2} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} X^1 + b^{l,2}) & W^{l,2} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}hidden}} & b^{l,2} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}batch}\\times \\mathsf{\\vphantom{fg}chans}[c_0] \\times \\mathsf{\\vphantom{fg}height}\\times \\mathsf{\\vphantom{fg}width}} \\\\\n",
    "T^1 &= \\text{relu}(\\text{Conv}^1(X^0)) \\\\\n",
    "X^1 &= \\text{MaxPool}^1(T^1) \\\\\n",
    "T^2 &= \\text{relu}(\\text{Conv}^2(X^1)) \\\\\n",
    "X^2 &= \\text{MaxPool}^2(T^2)_{(\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}width},\\mathsf{\\vphantom{fg}chans})\\rightarrow\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "X^3 &= \\text{relu}(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}layer}} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "O &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}classes}}}{\\vphantom{fg}\\mathrm{softmax}}} (W^4 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} X^3 + b^4) & W^4 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}classes}\\times \\mathsf{\\vphantom{fg}hidden}} & b^4 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}classes}}\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^2 &= \\text{MaxPool}^2(T^2) \\\\\n",
    "X^3 &= \\text{relu}(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}\\\\ \\mathsf{\\vphantom{fg}width}\\\\ \\mathsf{\\vphantom{fg}chans}}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}height}\\times \\mathsf{\\vphantom{fg}width}\\times \\mathsf{\\vphantom{fg}chans}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Conv}^l(X) &= \\text{Conv2d}(X; W^l, b^l)_{\\mathsf{\\vphantom{fg}chans2}\\rightarrow\\mathsf{\\vphantom{fg}chans}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W^l & \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans2}[c_l] \\times \\mathsf{\\vphantom{fg}chans}[c_{l-1}] \\times \\mathsf{\\vphantom{fg}kh}[kh_l] \\times \\mathsf{\\vphantom{fg}kw}[kw_l]} \\\\\n",
    "b^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans2}[c_l]}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MaxPool}^l(X) &amp;= \\text{MaxPool2d}_{ph^l,ph^l}(X).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@defop\n",
    "def Relu(X: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.maximum(X, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexable(tensor([[[0.0000, 0.0000, 0.7743, 0.4003, 0.0307],\n",
       "                   [0.6207, 0.7383, 1.5634, 2.1957, 1.1405],\n",
       "                   [0.0000, 0.9951, 0.0212, 0.0000, 0.2615]],\n",
       "          \n",
       "                  [[0.0000, 1.7244, 0.2876, 0.0000, 0.0000],\n",
       "                   [1.9150, 1.6981, 0.1880, 0.0000, 0.0000],\n",
       "                   [1.0409, 2.6747, 0.0000, 0.0000, 0.0000]],\n",
       "          \n",
       "                  [[0.0000, 0.3737, 0.6651, 0.0000, 0.0000],\n",
       "                   [0.6258, 0.8624, 0.1646, 2.0538, 0.0000],\n",
       "                   [0.0000, 0.7900, 0.0000, 0.0231, 0.0460]],\n",
       "          \n",
       "                  [[1.4793, 1.9515, 0.0000, 0.4130, 0.5423],\n",
       "                   [0.1468, 0.0000, 0.0000, 0.6351, 0.3265],\n",
       "                   [0.0000, 1.0214, 0.0000, 0.7716, 1.3249]]]))[batch(), chans(), layer()]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv2d() missing 1 required positional argument: 'width2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 57\u001b[0m\n\u001b[1;32m     52\u001b[0m b4 \u001b[38;5;241m=\u001b[39m Indexable(torch\u001b[38;5;241m.\u001b[39mrandn(classes_size))[classes()]\n\u001b[1;32m     53\u001b[0m X0 \u001b[38;5;241m=\u001b[39m Indexable(torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, chans_size, height_size, width_size))[\n\u001b[1;32m     54\u001b[0m     batch(), chans(), height(), width()\n\u001b[1;32m     55\u001b[0m ]\n\u001b[0;32m---> 57\u001b[0m T1 \u001b[38;5;241m=\u001b[39m Relu(\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m X1 \u001b[38;5;241m=\u001b[39m MaxPool2d(T1, height2, \u001b[38;5;241m3\u001b[39m, kh, height3, width2, \u001b[38;5;241m3\u001b[39m, kw, width3)\n\u001b[1;32m     59\u001b[0m X3 \u001b[38;5;241m=\u001b[39m reduce([height3, width3, chans2], W3 \u001b[38;5;241m*\u001b[39m X1, torch\u001b[38;5;241m.\u001b[39msum) \u001b[38;5;241m+\u001b[39m b3\n",
      "File \u001b[0;32m~/development/effectful/effectful/ops/types.py:60\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meffectful\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_interpretation\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meffectful\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemantics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__default_rule__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_interpretation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/effectful/effectful/internals/base_impl.py:54\u001b[0m, in \u001b[0;36m_BaseOperation.__default_rule__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meffectful\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDefaultRule\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoDefaultRule:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__free_rule__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/development/effectful/effectful/ops/semantics.py:27\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(intp, op, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m intp[apply](intp, op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__default_rule__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/effectful/effectful/internals/base_impl.py:54\u001b[0m, in \u001b[0;36m_BaseOperation.__default_rule__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meffectful\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDefaultRule\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoDefaultRule:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__free_rule__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Conv2d() missing 1 required positional argument: 'width2'"
     ]
    }
   ],
   "source": [
    "(\n",
    "    chans_size,\n",
    "    kh_size,\n",
    "    kw_size,\n",
    "    hidden_size,\n",
    "    height_size,\n",
    "    width_size,\n",
    "    classes_size,\n",
    "    batch_size,\n",
    ") = (3, 3, 4, 3, 14, 15, 5, 4)\n",
    "(\n",
    "    chans,\n",
    "    chans2,\n",
    "    kh,\n",
    "    kw,\n",
    "    height,\n",
    "    height2,\n",
    "    height3,\n",
    "    width,\n",
    "    width2,\n",
    "    width3,\n",
    "    hidden,\n",
    "    classes,\n",
    "    classes2,\n",
    "    batch,\n",
    ") = gensyms(\n",
    "    \"chans\",\n",
    "    \"chans2\",\n",
    "    \"kh\",\n",
    "    \"kw\",\n",
    "    \"height\",\n",
    "    \"height2\",\n",
    "    \"height3\",\n",
    "    \"width\",\n",
    "    \"width2\",\n",
    "    \"width3\",\n",
    "    \"hidden\",\n",
    "    \"classes\",\n",
    "    \"classes2\",\n",
    "    \"batch\",\n",
    ")\n",
    "\n",
    "W1 = Indexable(torch.randn(chans_size, kh_size, kw_size, chans_size))[\n",
    "    chans(), kh(), kw(), chans2()\n",
    "]\n",
    "b1 = Indexable(torch.randn(chans_size))[chans2()]\n",
    "W3 = Indexable(torch.randn(hidden_size, 4, 4, chans_size))[\n",
    "    hidden(), height3(), width3(), chans2()\n",
    "]\n",
    "b3 = Indexable(torch.randn(hidden_size))[hidden()]\n",
    "W4 = Indexable(torch.randn(hidden_size, classes_size))[hidden(), classes()]\n",
    "b4 = Indexable(torch.randn(classes_size))[classes()]\n",
    "X0 = Indexable(torch.randn(batch_size, chans_size, height_size, width_size))[\n",
    "    batch(), chans(), height(), width()\n",
    "]\n",
    "\n",
    "T1 = Relu(Conv2d(X0, W1, b1, chans, kh_size, kh, height, height2, kw, width, width2))\n",
    "X1 = MaxPool2d(T1, height2, 3, kh, height3, width2, 3, kw, width3)\n",
    "X3 = reduce([height3, width3, chans2], W3 * X1, torch.sum) + b3\n",
    "O = Softmax(reduce([hidden], W4 * X3, torch.sum) + b4, classes, classes2)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
