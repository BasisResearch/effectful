{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089a522-7a04-4623-889d-15c3b1b241fa",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from effectful.handlers.llm import Template\n",
    "from effectful.handlers.llm.providers import OpenAIAPIProvider, tool_call\n",
    "from effectful.handlers.llm.structure import DecodeError, decode\n",
    "from effectful.handlers.llm.synthesis import ProgramSynthesis\n",
    "from effectful.ops.semantics import fwd, handler\n",
    "from effectful.ops.syntax import defop\n",
    "\n",
    "provider = OpenAIAPIProvider(openai.OpenAI(api_key=os.environ['OPENAI_API_KEY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb93430",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def limerick(theme: str) -> str:\n",
    "    \"\"\"Write a limerick on the theme of {theme}\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe55e9f",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where numbers convene,  \n",
      "Floats dance on a binary sheen.  \n",
      "They wobble with flair,  \n",
      "Precision to spare,  \n",
      "But sometimes they skip what's unseen.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with handler(provider):\n",
    "    print(limerick(\"floating point computations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f072f",
   "metadata": {},
   "source": [
    "Now we're going to encode some examples from the pocketflow cookbook into the effectful-llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38691844",
   "metadata": {},
   "source": [
    "### Pocketflow Agent\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent)\n",
    "\n",
    "Idea: a state-machine, with the actions, decide (between searching, answer), search, answer\n",
    "\n",
    "```mermaid\n",
    "stateDiagram-v2\n",
    "    [*] --> Decide\n",
    "    Decide --> Search : search\n",
    "    Decide --> Answer : answer\n",
    "    Search --> Decide\n",
    "    Answer --> [*]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03938d5a",
   "metadata": {},
   "source": [
    "in pocketflow this is encoded using this class node, which states subclass and implement three methods for (`prep`, `exec`, `post`).\n",
    "\n",
    "`prep` - takes information from a shared context\n",
    "`exec` - takes this information and feeds it to an LLM call \n",
    "`post` - stores the LLM call into the shared context, and returns the next action\n",
    "\n",
    "For example:\n",
    "```python\n",
    "class DecideAction(Node):\n",
    "    def prep(self, shared):\n",
    "        context = shared.get('context') or ''\n",
    "        question = shared['question']\n",
    "        return question, context\n",
    "    \n",
    "    def exec(self, inputs):\n",
    "        question, context = inputs\n",
    "        prompt = \"\"\" you are a research assistant that can search the web, ... \"\"\"\n",
    "        res = call_llm(prompt)\n",
    "        return decode(res)\n",
    "    \n",
    "    def post(self, shared, res):\n",
    "        shared['context'] = exec_res['answer']\n",
    "        return result['action']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa905b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is: The question about the \"meaning of life\" is a profound and philosophical one asked throughout history. It doesn't have a single definitive answer but varies across different cultures, philosophies, and individual beliefs. Here are some perspectives:\n",
      "\n",
      "1. **Religious Perspectives**: Many religions offer answers, suggesting the meaning of life is to fulfill the will of a divine creator or to achieve a spiritual goal, such as salvation in Christianity, enlightenment in Buddhism, or union with God in Islam.\n",
      "\n",
      "2. **Philosophical Views**:\n",
      "    - **Existentialism**: Suggests life has no inherent meaning, and it is up to each individual to create their own purpose and meaning.\n",
      "    - **Nihilism**: Argues life is without objective meaning, purpose, or intrinsic value.\n",
      "    - **Absurdism**: Proposes that humans naturally seek meaning, but life is intrinsically without it, creating an inherent conflict.\n",
      "\n",
      "3. **Scientific Perspectives**: From a scientific view, the meaning of life is often connected to the survival and reproduction of the species. Some align with the idea that life is a complex process arising from natural phenomena.\n",
      "\n",
      "4. **Humanistic Views**: Focus on human welfare and the pursuit of individual fulfillment and social connections as central to finding meaning in life.\n",
      "\n",
      "5. **Cultural and Personal Perspectives**: Different cultures and individuals derive meaning from their relationships, experiences, and societal roles, creating a mosaic of meanings.\n",
      "\n",
      "Ultimately, the search for meaning is a personal journey, influenced by individual experiences, beliefs, and aspirations.\n"
     ]
    }
   ],
   "source": [
    "from effectful.ops.syntax import ObjectInterpretation, implements\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "\n",
    "@defop\n",
    "def search_web(query: str) -> str:\n",
    "    \"Search the web for a query string and return the results.\"\n",
    "    raise NotHandled\n",
    "\n",
    "class DuckDuckGoSearchProvider(ObjectInterpretation):\n",
    "    \"\"\"Implements web search using DuckDuckGo.\"\"\"\n",
    "    @implements(search_web)\n",
    "    def search_web(self, query: str) -> str:\n",
    "        url = f\"https://api.duckduckgo.com/?{urlencode({'q': query, 'format': 'json', 'pretty': '1'})}\"\n",
    "        headers = {\"accept\": 'application/json'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code in {202, 200}:\n",
    "            results = response.json()['RelatedTopics']\n",
    "            return '\\n\\n'.join([\n",
    "                f\"URL: {r.get('FirstURL')}\\nDescription: {r.get('Text')}\"\n",
    "                for r in results if 'FirstURL' in r and 'Text' in r\n",
    "            ])\n",
    "        else: return f'Request failed with status code {response.status_code}'\n",
    "\n",
    "\n",
    "@Template.define(tools=[search_web])\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"Acting as a Research Assistant that can search the web, construct an answer to the user's question, {question}.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "# feels weird to  pass in tools at the place of definition --- what if I want the agent to support more tools? it might feel nicer to give tools seperately\n",
    "@Template.define(tools=[search_web])\n",
    "def refine_answer(question: str, answer: str) -> str:\n",
    "    \"\"\"Acting as a Research Assistant that can search the web, given the user's original question, {question}, refine this previous answer {answer}.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "\n",
    "# would like to refine the prompt based on type signature, currently I need to give this instruction\n",
    "@Template.define\n",
    "def is_question_answered(question: str, answer: str) -> bool:\n",
    "    \"\"\"Acting as a Research Assistant, Decide if the user's question {question} is appropriately answered by {answer}.\n",
    "    <instruction>\n",
    "    respond only true or false. Do not give any explanations.\n",
    "    </instruction>\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "def research_agent(question: str, max_attempts: int = 3) -> str:\n",
    "    \"\"\"A research agent which answers a question\"\"\"\n",
    "    answer = answer_question(question)\n",
    "    \n",
    "    for _ in range(max_attempts):\n",
    "        if is_question_answered(question, answer):\n",
    "            break\n",
    "        answer = refine_answer(question, answer)\n",
    "            \n",
    "    return answer\n",
    "\n",
    "with handler(provider), handler(DuckDuckGoSearchProvider()):\n",
    "    meaning_of_life = research_agent('what is the meaning of life?')\n",
    "    print(f'The meaning of life is: {meaning_of_life}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ba36d",
   "metadata": {},
   "source": [
    "Given the repetition of \"Acting as a research agent\" and the fact that these functions are conceptually tied together and dependent, it might make sense to wrap them up in an object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent:\n",
    "    \"\"\"research Agent that answers user's questions\"\"\"\n",
    "    tools: List[Callable]\n",
    "    max_attempts: int\n",
    "    def __init__(self, max_attempts: int, tools=[search_web]):\n",
    "        self.tools = tools\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    @Template.define(tools=tools)\n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Construct an answer to the user's question {question}\"\"\"\n",
    "        ...\n",
    "    \n",
    "    @Template.define(tools=tools)\n",
    "    def refine_answer(self, question: str, answer: str) -> str:\n",
    "        \"\"\"Given the user's original question, {question}, refine this previous answer: {answer}\"\"\"\n",
    "        ...\n",
    "\n",
    "    @Template.define\n",
    "    def is_question_answered(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Decide if the user's question {question} is appropriately answered by {answer}.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        answer = self.answer_question(question)\n",
    "        for _ in range(self.max_attempts):\n",
    "            if self.is_question_answered(question, answer):\n",
    "                break\n",
    "            answer = refine_answer(question, answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a877cf",
   "metadata": {},
   "source": [
    "### PocketFlow Async Basic example\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-async-basic)\n",
    "\n",
    "Same model as above, but using async nodes\n",
    "```python\n",
    "class FetchRecipies:\n",
    "    async def prep(self, shared):\n",
    "        return shared['ingredient']\n",
    "    async def exec(self, ingredient):\n",
    "        return await fetch_recipies(ingredient)\n",
    "    async def post(self, shared, res):\n",
    "        shared['recipes'] = res\n",
    "        return \"suggest\"\n",
    "class SuggestRecipe:\n",
    "    async def prep(self, shared):\n",
    "        return shared['recipes']\n",
    "    async def exec(self, recipes):\n",
    "        suggestion = await call_llm(\"Choose the best recipe from {','.join(recipes)}\")\n",
    "        return suggestion\n",
    "    async def post(self, shared, res):\n",
    "        shared['suggestion'] = suggestion\n",
    "        return \"approve\"\n",
    "class GetApproval:\n",
    "    async def prep(self, shared): return shared['suggestion']\n",
    "    async def exec(self, suggestion): return await input('Accept this recipe?')\n",
    "    async def post(self, shared, answer):\n",
    "        if answer == \"y\":\n",
    "            return \"accept\"\n",
    "        else:\n",
    "            return \"retry\"\n",
    "```\n",
    "This could probably be encoded using the same structure as above. I assume this is trivial, so left out, but should be a similar design to the previous, just with async, it doesn't really make use of the async part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def suggest_best_recipe(recipes: list[str]) -> str:\n",
    "    \"\"\"Return the best recipe from this list <instruction>return only the recipe, do not give any explanation.</instruction>\"\"\"\n",
    "\n",
    "async def find_recipes(initial_ingredient: str) -> list[str]:\n",
    "    found_recipe = None\n",
    "    while not found_recipe:\n",
    "        recipes = await fetch_recipes_using(ingredient)\n",
    "        found_recipe = await suggest_best_recipe(recipes)\n",
    "        print(found_recipe)\n",
    "        if input('Accept this recipe?') == 'y':\n",
    "            break\n",
    "    return found_recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c192d",
   "metadata": {},
   "source": [
    "### Pocketflow BatchFlow example\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch-flow).\n",
    "\n",
    "Introduces the notion of parameters, which allows batching flows over several choices of parameter\n",
    "\n",
    "```python\n",
    "class LoadImage:\n",
    "    def prep(self, shared): return os.path.join(\"images\", self.params['input'])\n",
    "    def exec(self, image_path): return Image.load(image_path)\n",
    "    def post(self, shared, res): shared[\"image\"] = res; return \"apply_filter\"\n",
    "\n",
    "class ApplyFilter:\n",
    "    def prep(self, shared): return shared['image'], self.params['filter']\n",
    "    def exec(self, inputs):\n",
    "        image, filter_type = inputs\n",
    "        match filter_type: ...\n",
    "    def post(self, shared, res):\n",
    "        shared['filtered_image'] = res\n",
    "        return \"save\"\n",
    "class SaveImage:\n",
    "    def prep(self, shared): \n",
    "        os.makedirs('output')\n",
    "        input_name, filter_name, output_path = ...\n",
    "        return shared['filtered_image'], output_path\n",
    "    def exec(self, inputs): image, output_path = inputs; image.save(output_path, \"jpeg\"); return output_path\n",
    "    def post(self, shared, res): return \"default\"\n",
    "```\n",
    "\n",
    "Batching allows taking a single flow and runnning it over a series of parameters\n",
    "```python\n",
    "class ImageBatchFlow(BatchFlow):\n",
    "    def prep(self, shared):\n",
    "        images = ['cat.jpeg', 'dog.jpeg', 'bird.jpeg']\n",
    "        filters = ['grayscale', 'blur', 'sepia']\n",
    "        params = []\n",
    "        for img in images: for filter in filters: params.append({'input':img, 'filter': f})\n",
    "        return params\n",
    "```\n",
    "\n",
    "No real LLM stuff here, can be represented with a function I suppose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a00d1",
   "metadata": {},
   "source": [
    "### Pocketflow Batch\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch)\n",
    "Demonstrates batching - idea is subclassing batchnode means pre and post need to handle batches, the exec is written as if straight line unbatched\n",
    "\n",
    "```python\n",
    "\n",
    "class TranslateTextNode(BatchNode):\n",
    "    def prep(self, shared):\n",
    "        text, languages = shared[\"text\"], shared['languages']\n",
    "        return [(text,lang) for lang in langauges]\n",
    "    def exec(self, data): text, language = data; return call_llm(f\"translate the following into {language}: {text}\")\n",
    "    def post(self, shared, ress):\n",
    "        for res in ress: os.write(res['language'], res['translation'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "949ab9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment ça va ? Comment se passe ta journée ?\n",
      "\n",
      "© 2023 OpenAI\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "@Template.define\n",
    "def translate(text: str, language: str) -> str:\n",
    "    \"\"\"Translate the provided text into {language}: {text}\"\"\"\n",
    "\n",
    "def instructions(*instructions: list[str]):\n",
    "    instructions = '\\n'.join(instructions)\n",
    "    class InstructionsInterpretation(ObjectInterpretation):\n",
    "        @implements(Template.__call__)        \n",
    "        def _call(self, template, *args, **kwargs) -> None:\n",
    "            prompt_ext = (f\"{template.__prompt_template__}\\n<instructions>\\n{instructions}\\n</instructions>\")\n",
    "            return fwd(dataclasses.replace(template, __prompt_template__=prompt_ext), *args, **kwargs)\n",
    "    return handler(InstructionsInterpretation())\n",
    "            \n",
    "\n",
    "\n",
    "with handler(provider), instructions(\"Do not give any explanation\", \"Add a copyright note\"):\n",
    "    print(translate('hello, how are you? how is your day going?', 'french'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05404c",
   "metadata": {},
   "source": [
    "### Travel Advisor Chat with Guardrails\n",
    "\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail)\n",
    "\n",
    "```python \n",
    "class GuardrailNode:\n",
    "    def prep(self, shared):\n",
    "        return shared['user_input']\n",
    "    def exec(self, user_input):\n",
    "        return call_llm('evaluate if the following query is travel related ...') \n",
    "    def post(self, shared, is_valid):\n",
    "        if not is_vaild: return \"retry\"\n",
    "        shared[\"messages\"].append({'role': \"user\", \"content\": shared['user_input']})\n",
    "        return \"process\"\n",
    "```\n",
    "\n",
    "maybe this could be encoded using handlers? (playing a bit fast and loose with the exact definitions here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54904dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out Central Park, Times Square, Statue of Liberty, Brooklyn Bridge, Empire State Building, 9/11 Memorial, Metropolitan Museum of Art, Broadway, Rockefeller Center, and The High Line.\n",
      "Not asking question Should I buy apple stocks? as it is not related to travel advice.\n"
     ]
    }
   ],
   "source": [
    "@Template.define\n",
    "def travel_query(user_query: str) -> str:\n",
    "    \"\"\"Produce a concise (<100) word answer to the travel query {user_query}\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "@Template.define\n",
    "def is_safe_query(user_query: str) -> bool:\n",
    "    \"\"\"Determine whether the user's query {user_query} is purely related to travel advice.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "def answer_travel_query(user_query: str):\n",
    "    with instructions(\"Respond only true or false\"):\n",
    "        is_safe = is_safe_query(user_query)\n",
    "    if not is_safe:\n",
    "        return f'Not asking question {user_query} as it is not related to travel advice.'\n",
    "    else:\n",
    "        return travel_query(user_query)\n",
    "\n",
    "with handler(provider), instructions(\"Do not give any explanation\"):\n",
    "    print(answer_travel_query(\"What are great places to check out in NYC?\"))\n",
    "    print(answer_travel_query('Should I buy apple stocks?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def is_safe_query(user_query: str) -> bool:\n",
    "    \"\"\"Tests whether the user query {user_query} is related to travel advice\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "@Template.define\n",
    "def travel_query(user_query: Anotated[str, is_safe_query]) -> str:\n",
    "    \"\"\"Produces a concise (<100) word answer to the query {user_query}\"\"\"\n",
    "    raise NotHandled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa250240",
   "metadata": {},
   "source": [
    "@Template.define\n",
    "def travel_query(user_query: str):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a66b6e",
   "metadata": {},
   "source": [
    "### PocketFlow Chat with Memory\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory)\n",
    "\n",
    "```python \n",
    "class GetUserQuestionNode:\n",
    "    def prep(self, shared): shared['messages'] = shared.get('messages', [])\n",
    "    def exec(self, _): return input()\n",
    "    def post(self, shared, res): shared['messages'].append({'role': 'user', 'content': exec_res}); return \"retrieve\"\n",
    "\n",
    "class AnswerNode:\n",
    "    def prep(self, shared):\n",
    "        recent_messages = shared.get('messages', [])[-6:]\n",
    "        context = []\n",
    "        if (relevant_convos := shared.get('retrieved_conversation')):\n",
    "            context.append({'role': 'system', 'content': 'the following is a relevant past conversation that might help'})\n",
    "            context.extend(relevant_convos)\n",
    "        context.extend(recent_messages)\n",
    "        context.append({'role': 'system', 'content': 'now continue the conversation'})\n",
    "        return context\n",
    "    def exec(self, messages): return call_llm(messages)\n",
    "    def post(self, shared, res):\n",
    "        shared['messages'].append({'role': 'assistant', 'content': res})\n",
    "        if len(shared['messages']) > 6: return 'embed'\n",
    "        return 'question'\n",
    "\n",
    "class EmbedNode:\n",
    "    def prep(self, shared):\n",
    "        oldest_pair, shared['messages'] = shared['messages'][:2], shared['messages'][2:]\n",
    "        return oldest_pair\n",
    "    def exec(conversation):\n",
    "        embedding = get_embedding('\\n'.join(msg['content'] for msg in conversation))\n",
    "        return {'conversation': conversation, 'embedding': embedding}\n",
    "    def post(self, shared, res):\n",
    "        add_vector(shared['vector_index'], (res['embedding']))\n",
    "        return 'question'\n",
    "\n",
    "class RetrieveNode:\n",
    "    def prep(self, shared): return next(msg for msg in shared['messages'] if msg['role'] == 'user', None)\n",
    "    def exec(self, input): \n",
    "        query = inputs['query']; vector_index = inputs['vector_index']; vector_items = inputs['vector_items']\n",
    "        [index], [dist] = search_vectors(inputs['vector_index'], get_embedding(inputs['query']), k = 1)\n",
    "        return {'conversation': inputs['vector_items'][index], 'distance': dist}\n",
    "    def post(self, shared, res):\n",
    "        shared['retrieved_conversation'] = res['conversation']\n",
    "        return 'answer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b201d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: hello!, how are you doing?\n",
      "agent: Hello! I'm just a program, but I'm here to help you. How's your day going?\n",
      "user: lovely! I'm having a lovely day\n",
      "agent: That’s wonderful to hear! What’s been the highlight of your day so far?\n",
      "user: What is the captial of france?\n",
      "agent: agent: The capital of France is Paris. Is there anything else you'd like to know?\n",
      "user: I didn't know that! That's amazing!\n",
      "agent: agent: I'm glad you found it amazing! Is there anything else you're curious about?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = provider._client.embeddings.create(model=\"text-embedding-ada-002\", input=text)\n",
    "    return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "\n",
    "def find_closest(index: list[tuple[str, Any]], phrase):\n",
    "    if not index: return None\n",
    "    def dist(a, b): return float(((a - b) ** 2).sum())\n",
    "    phrase_embedding = get_embedding(phrase)\n",
    "    return min(((msg, dist(embedding, phrase_embedding)) for (msg, embedding) in index), key=lambda elt: elt[1])\n",
    "\n",
    "@Template.define\n",
    "def respond_to_user(user_message: str, relevant_context: str, prev_messages: str) -> str:\n",
    "    \"\"\"Given the user wrote {user_message}, continue the conversation. The last few messages in the conversation were {prev_messages}, and older context was {relevant_context}\"\"\"\n",
    "    ...\n",
    "\n",
    "class ChatAgent:\n",
    "    history: list[str] = []\n",
    "    index = []\n",
    "    \n",
    "    def _compress(self):\n",
    "        oldest_pair, self.history = self.history[:2], self.history[2:]\n",
    "        oldest_pair = '\\n'.join(message['content'] for message in oldest_pair)\n",
    "        self.index.append([oldest_pair, get_embedding(oldest_pair)])\n",
    "    \n",
    "    def _find_relevant(self, query: str):\n",
    "        return find_closest(self.index, query)\n",
    "\n",
    "\n",
    "    def chat(self, input: str):\n",
    "        relevant = self._find_relevant(input)\n",
    "        relevant_context = relevant[0] if relevant else 'No relevant context.'\n",
    "        prev_messages = '\\n'.join([f\"{message['author']}: {message['content']}\" for message in self.history])\n",
    "        response = respond_to_user(input, relevant_context, prev_messages)\n",
    "        self.history.append({'author': 'user', 'content': input})\n",
    "        self.history.append({'author': 'agent', 'content': response})\n",
    "        if len(self.history) > 6: self._compress()\n",
    "        print(f'user: {input}')\n",
    "        print(f'agent: {response}')\n",
    "\n",
    "agent = ChatAgent()\n",
    "with handler(provider), instructions('Do not give any explanation, simply return the response.'):\n",
    "    agent.chat('hello!, how are you doing?')\n",
    "    agent.chat('lovely! I\\'m having a lovely day')\n",
    "    agent.chat('What is the captial of france?')\n",
    "    agent.chat('I didn\\'t know that! That\\'s amazing!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bc956",
   "metadata": {},
   "source": [
    "### Pocketflow LLM Streaming\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming)\n",
    "\n",
    "```python\n",
    "class StreamNode:\n",
    "    def prep(self, shared):\n",
    "        interrupt_evt = threading.Event()\n",
    "        def wait_for_interrupt(): interrupt_evt.set()\n",
    "        listener_thread = threading.Thread(target = wait_for_interrupt)\n",
    "        listener_thread.start()\n",
    "        chunks = stream_llm(shared['prompt'])\n",
    "        return chunks, interrupt_evt, listener_thread\n",
    "    def exec(self, inputs):\n",
    "        chunks, interrupt_evt, listener_thread = inputs\n",
    "\n",
    "        for chunk in chunks:\n",
    "            if interrupt_evt.is_set():\n",
    "                break\n",
    "            print(chunk.choices[0].delta.content)\n",
    "        \n",
    "        return interrupt_evt, listenker_thread\n",
    "    def post(self, shared, res):\n",
    "        interrupt_evt, listener_thread = res \n",
    "        interrupt_evt.set(); listener_thread.join()\n",
    "        return 'default'\n",
    "```\n",
    "\n",
    "Probably good to support this, though Pocket-flow doesn't really have the nicest interface here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88533caf",
   "metadata": {},
   "source": [
    "### Pocketflow Majority vote\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote)\n",
    "\n",
    "illustrates how to use the batching mechanism to implement majority voting\n",
    "\n",
    "```python\n",
    "class MajorityVote(BatchNode):\n",
    "    def prep(self, shared):\n",
    "        question = shared['question']\n",
    "        attempts = shared.get('num_tries', 3)\n",
    "        return [question for _ in range(attempts)]\n",
    "\n",
    "    def exec(self, question):\n",
    "        return call_llm(\"Please answer the user's question below\\n{question},answer\")\n",
    "\n",
    "    def post(self, shared, ress):\n",
    "        answers = [res['answer'] for res in ress]\n",
    "        best_answer, freq = Counter(answers).most_common(1)[0]\n",
    "        return 'end'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00d4000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yes.', 3)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def majority_vote(oracle, query, voters=3):\n",
    "    with instructions('respond only yes or no'):\n",
    "        counter = collections.Counter([oracle(query) for i in range(voters)])\n",
    "    best_answer, freq = counter.most_common(1)[0]\n",
    "    return best_answer, freq\n",
    "\n",
    "with handler(provider):\n",
    "    print(majority_vote(travel_query, 'Is paris the captial of france?'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd2749",
   "metadata": {},
   "source": [
    "### Pocketflow Multi-Agent Game\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent)\n",
    "\n",
    "Async communication between two nodes\n",
    "\n",
    "```python\n",
    "class AsyncHinter(AsyncNode):\n",
    "    async def prep(self, shared):\n",
    "        guess = await shared['hinter_queue'].get()\n",
    "        if guess == 'GAME_OVER': return None\n",
    "        return shared['target_word'], shared['forbidden_words'], shared.get('past_guesses', [])\n",
    "    \n",
    "    async def exec(self, inputs):\n",
    "        target, forbidden, past_guesses = inputs\n",
    "        prompt = 'generate hint for {target}, forbidden words: {forbidden}, past guesses were: {past_guesses}'\n",
    "        hint = call_llm(prompt)\n",
    "        return hint\n",
    "    \n",
    "    async def post(self, shared, res):\n",
    "        if res is None:\n",
    "            return 'end'\n",
    "        await shared['guesser_queue'].put(exec_res)\n",
    "        return 'continue'\n",
    "    \n",
    "class AsyncGuesser(AsyncNode):\n",
    "    async def prep(self, shared):\n",
    "        hint = await shared['guesser_queue'].get()\n",
    "        return hint, shared.get('past_guesses', [])\n",
    "    \n",
    "    async def exec(self, inputs):\n",
    "        hint, past_guesses = inputs\n",
    "        prompt = 'given hint {hint}, past guesses {past_guesses}, make a new guess'\n",
    "        guess = call_llm(prompt)\n",
    "        return guess\n",
    "    \n",
    "    async def post(self, shared, res):\n",
    "        if res == shared['target_word']:\n",
    "            await shared['hinter_queue'].put('GAME_OVER')\n",
    "            return 'end'\n",
    "        shared['past_guesses'].append(exec_res)\n",
    "        await shared['hinter_queue'].put(exec_res)\n",
    "        return 'continue'\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "async def guesser(hint, wrong_guesses):\n",
    "    \"\"\"Given the hint {hint}, guess the hidden word. Prior wrong guesses were {wrong_guesses}\"\"\"\n",
    "    ...\n",
    "\n",
    "@Template.define\n",
    "async def hinter(guessed_word, hint, hidden_word, wrong_guesses):\n",
    "    \"\"\"Given the wrong guess {guessed_word} for hint {hint}, and prior wrong guesses ({wrong_guesses}), construct a new hint.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class Guesser:\n",
    "    prior_guesses = []\n",
    "    last_guess = ''\n",
    "    last_hint = ''\n",
    "    \n",
    "    async def make_guess(self, hint: str) -> str:\n",
    "        guess = guesser(hint, self.prior_guesses)\n",
    "        self.last_guess = guess\n",
    "        self.last_hint = hint\n",
    "        return guess\n",
    "    \n",
    "    async def report_wrong(self):\n",
    "        self.prior_guesses.append(self.last_guess)\n",
    "\n",
    "async def evaluate_guesser(guesser: Guesser, hidden_word):\n",
    "    hint = hinter(guesser.last_guess, guesser.hint, hidden_word, guesser.prior_guesses)\n",
    "    guess = await guesser.make_guess(hint)\n",
    "    if guess.lower() == hidden_word.lower():\n",
    "        return True\n",
    "    guesser.report_wrong()\n",
    "    return False\n",
    "\n",
    "\n",
    "async def run_guessing_game(guesser, hidden_word):\n",
    "    while not await evaluate_guesser(guesser, hidden_word):\n",
    "        continue\n",
    "    \n",
    "\n",
    "with handler(provider):\n",
    "    guessing_games = [run_guessing_game(Guesser()) for i in range(10)]\n",
    "    finished_game = await asyncio.wait(guessing_games, return_when=asyncio.FIRST_COMPLETED)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439cec3",
   "metadata": {},
   "source": [
    "### Pocketflow parallel batch flow\n",
    "\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow)\n",
    "\n",
    "same code as batched, just changes execution strategy such that each flow is run in parallel\n",
    "```python\n",
    "class ImageParallelBatchFlow(AsyncParallelBatchFlow):\n",
    "    \n",
    "    async def prep_async(self, shared):\n",
    "        images = shared.get(\"images\",[])\n",
    "        filters = ['grayscale', 'blur', 'sepia']\n",
    "        params = []\n",
    "        for image_path in images:\n",
    "            for filter_type in filters:\n",
    "                params.append({ 'image_path': image_path, 'filter': filter_type })\n",
    "        return params\n",
    "```\n",
    "\n",
    "```python\n",
    "class TranslateTextNodeParallel(AsyncParallelBatchNode):\n",
    "    async def prep_async(self, shared):\n",
    "        text = shared.get(\"text\", \"no text provided\")\n",
    "        languages = shared.get(\"langauges\", [])\n",
    "        return [(text,lang) for lang in languages]\n",
    "    async def exec_async(self, inputs):\n",
    "        text, language = inputs\n",
    "        result = await call_llm(\"please translate the following markdown file into {language}\")\n",
    "        return {\"language\": language, \"translation\": result}\n",
    "    async def post_async(self, shared, res):\n",
    "        for file in res:\n",
    "            with open(file['language'], 'w') as f: await f.write(file['translation'])\n",
    "        return 'default'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457cae4",
   "metadata": {},
   "source": [
    "### Pocketflow RAG example\n",
    "\n",
    "```python\n",
    "class ChunkDocumentsNode(BatchNode):\n",
    "    def prep(self,shared): return shared['texts']\n",
    "    def exec(self, text): return fixed_size_chunk(text)\n",
    "    def post(self,shared,res): shared['texts'] = [doc for doc_ls in res for doc in doc_ls]; return 'default'\n",
    "class EmbedDocumentsNode(BatchNode):\n",
    "    def prep(self,shared): return shared['texts']\n",
    "    def exec(self,text): return get_embedding(text)\n",
    "    def post(self,shared, res): shared['embeddings'] = np.array(res); return 'default'\n",
    "class CreateIndexNode(Node):\n",
    "    def prep(self, shared): return shared['query']\n",
    "    def exec(self,query): return get_embedding(query)\n",
    "    def post(Self,shared, res): shared['query_embedding'] = res\n",
    "class RetrieveDocumentsNode(Node):\n",
    "    def prep(self,shared): return shared['query_embedding'], shared['index'], shared['texts']\n",
    "    def exec(self,inputs): q_embed, index, texts = inputs; dists, inds = index.search(q_embed, k=1); return {'text': texts[inds[0][0]], 'ind': inds[0][0]}\n",
    "    def post(self,shared,res): shared['retrieved'] = res; return 'default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112d1bc",
   "metadata": {},
   "source": [
    "### Pocketflow Supervisor Flow\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-supervisor/flow.py)\n",
    "\n",
    "```python\n",
    "class SupervisorNode(Node):\n",
    "    def prep(self,shared): return shared['answer']\n",
    "    def exec(self,answer):\n",
    "        is_nonsense = any(marker in answer for marker in ['coffee break', 'who knows?', 'made up', '42'])\n",
    "        if is_nonsense: return {'valid': False, 'reason': 'Answer appears to be nonsense'}\n",
    "        else: return {'valid': True, 'reason': 'Answer appears to be legitimate'}\n",
    "    def post(self, shared, res):\n",
    "        if not res['valid']: return 'retry'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e53c9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effectful-llm (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "name": "Untitled.ipynb",
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
