{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8089a522-7a04-4623-889d-15c3b1b241fa",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from effectful.handlers.llm import Template\n",
    "from effectful.handlers.llm.providers import OpenAIAPIProvider, tool_call\n",
    "from effectful.handlers.llm.synthesis import ProgramSynthesis\n",
    "from effectful.ops.semantics import fwd, handler\n",
    "from effectful.ops.syntax import defop\n",
    "provider = OpenAIAPIProvider(openai.OpenAI(api_key=os.environ['OPENAI_API_KEY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb93430",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def limerick(theme: str) -> str:\n",
    "    \"\"\"Write a limerick on the theme of {theme}\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe55e9f",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the world of computing, so bright,  \n",
      "Float numbers can cause quite a fright.  \n",
      "With bits in a dance,  \n",
      "Precision's mere chance,  \n",
      "Yet we trust them to mostly be right!\n"
     ]
    }
   ],
   "source": [
    "with handler(provider):\n",
    "    print(limerick(\"floating point computations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f072f",
   "metadata": {},
   "source": [
    "Now we're going to encode some examples from the pocketflow cookbook into the effectful-llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38691844",
   "metadata": {},
   "source": [
    "### Pocketflow Agent\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent)\n",
    "\n",
    "Idea: a state-machine, with the actions, decide (between searching, answer), search, answer\n",
    "\n",
    "```mermaid\n",
    "stateDiagram-v2\n",
    "    [*] --> Decide\n",
    "    Decide --> Search : search\n",
    "    Decide --> Answer : answer\n",
    "    Search --> Decide\n",
    "    Answer --> [*]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03938d5a",
   "metadata": {},
   "source": [
    "in pocketflow this is encoded using this class node, which states subclass and implement three methods for (`prep`, `exec`, `post`).\n",
    "\n",
    "`prep` - takes information from a shared context\n",
    "`exec` - takes this information and feeds it to an LLM call \n",
    "`post` - stores the LLM call into the shared context, and returns the next action\n",
    "\n",
    "For example:\n",
    "```python\n",
    "class DecideAction(Node):\n",
    "    def prep(self, shared):\n",
    "        context = shared.get('context') or ''\n",
    "        question = shared['question']\n",
    "        return question, context\n",
    "    \n",
    "    def exec(self, inputs):\n",
    "        question, context = inputs\n",
    "        prompt = \"\"\" you are a research assistant that can search the web, ... \"\"\"\n",
    "        res = call_llm(prompt)\n",
    "        return decode(res)\n",
    "    \n",
    "    def post(self, shared, res):\n",
    "        shared['context'] = exec_res['answer']\n",
    "        return result['action']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa905b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is: The \"meaning of life\" is a question that has intrigued humanity for centuries, and various philosophical, religious, and scientific perspectives provide different interpretations. Here's a broad overview:\n",
      "\n",
      "1. **Philosophical Perspectives**:\n",
      "   - **Existentialism**: Suggests that life has no inherent meaning unless individuals create their own purpose through choices and actions. Prominent existentialists include Jean-Paul Sartre and Albert Camus.\n",
      "   - **Absurdism**: Coined by Camus, this perspective holds that humans seek meaning in a life that is inherently meaningless. Instead of despair, absurdism suggests embracing the absurdity.\n",
      "   - **Nihilism**: Claims that life is without objective meaning, purpose, or intrinsic value. While often seen as pessimistic, some argue it allows for freedom from prescribed values.\n",
      "\n",
      "2. **Religious Perspectives**:\n",
      "   - Many religions offer their own explanations. For example, Christianity proposes that life's purpose is to love and serve God and others, aligning with divine commandments.\n",
      "   - In Hinduism and Buddhism, the idea of karma and life cycles suggest that life's purpose is to achieve spiritual growth and eventually liberation (moksha or nirvana).\n",
      "\n",
      "3. **Scientific Perspectives**:\n",
      "   - The scientific view, especially from evolutionary biology, proposes that life's purpose is connected to survival and reproduction.\n",
      "   - Some scientists, like Richard Dawkins, argue that genes drive biological purposes, but individuals can transcend biology to pursue personal goals and altruism.\n",
      "\n",
      "4. **Personal and Subjective Experiences**:\n",
      "   - Many people find meaning in personal relationships, career achievements, artistic expression, and contributions to society. Viktor Franklâ€™s logotherapy highlights that meaning can be discovered through experiences of love, suffering, and work.\n",
      "\n",
      "Ultimately, the meaning of life is a deeply personal question, with each individual finding their own answers based on their beliefs, experiences, and reflections.\n"
     ]
    }
   ],
   "source": [
    "from effectful.ops.syntax import ObjectInterpretation, implements\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "\n",
    "@defop\n",
    "def search_web(query: str) -> str:\n",
    "    \"Search the web for a query string and return the results.\"\n",
    "    raise NotHandled\n",
    "\n",
    "class DuckDuckGoSearchProvider(ObjectInterpretation):\n",
    "    \"\"\"Implements web search using DuckDuckGo.\"\"\"\n",
    "    @implements(search_web)\n",
    "    def search_web(self, query: str) -> str:\n",
    "        url = f\"https://api.duckduckgo.com/?{urlencode({'q': query, 'format': 'json', 'pretty': '1'})}\"\n",
    "        headers = {\"accept\": 'application/json'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code in {202, 200}:\n",
    "            results = response.json()['RelatedTopics']\n",
    "            return '\\n\\n'.join([\n",
    "                f\"URL: {r.get('FirstURL')}\\nDescription: {r.get('Text')}\"\n",
    "                for r in results if 'FirstURL' in r and 'Text' in r\n",
    "            ])\n",
    "        else: return f'Request failed with status code {response.status_code}'\n",
    "\n",
    "\n",
    "@Template.define(tools=[search_web])\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"Acting as a Research Assistant that can search the web, construct an answer to the user's question, {question}.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "# feels weird to  pass in tools at the place of definition --- what if I want the agent to support more tools? it might feel nicer to give tools seperately\n",
    "@Template.define(tools=[search_web])\n",
    "def refine_answer(question: str, answer: str) -> str:\n",
    "    \"\"\"Acting as a Research Assistant that can search the web, given the user's original question, {question}, refine this previous answer {answer}.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "\n",
    "# would like to refine the prompt based on type signature, currently I need to give this instruction\n",
    "@Template.define\n",
    "def is_question_answered(question: str, answer: str) -> bool:\n",
    "    \"\"\"Acting as a Research Assistant, Decide if the user's question {question} is appropriately answered by {answer}.\n",
    "    <instruction>\n",
    "    respond only true or false. Do not give any explanations.\n",
    "    </instruction>\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "def research_agent(question: str, max_attempts: int = 3) -> str:\n",
    "    \"\"\"A research agent which answers a question\"\"\"\n",
    "    answer = answer_question(question)\n",
    "    \n",
    "    for _ in range(max_attempts):\n",
    "        if is_question_answered(question, answer):\n",
    "            break\n",
    "        answer = refine_answer(question, answer)\n",
    "            \n",
    "    return answer\n",
    "\n",
    "with handler(provider), handler(DuckDuckGoSearchProvider()):\n",
    "    meaning_of_life = research_agent('what is the meaning of life?')\n",
    "    print(f'The meaning of life is: {meaning_of_life}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ba36d",
   "metadata": {},
   "source": [
    "Given the repetition of \"Acting as a research agent\" and the fact that these functions are conceptually tied together and dependent, it might make sense to wrap them up in an object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent:\n",
    "    \"\"\"research Agent that answers user's questions\"\"\"\n",
    "    tools: List[Callable]\n",
    "    max_attempts: int\n",
    "    def __init__(self, max_attempts: int, tools=[search_web]):\n",
    "        self.tools = tools\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    @Template.define(tools=tools)\n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Construct an answer to the user's question {question}\"\"\"\n",
    "        ...\n",
    "    \n",
    "    @Template.define(tools=tools)\n",
    "    def refine_answer(self, question: str, answer: str) -> str:\n",
    "        \"\"\"Given the user's original question, {question}, refine this previous answer: {answer}\"\"\"\n",
    "        ...\n",
    "\n",
    "    @Template.define\n",
    "    def is_question_answered(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Decide if the user's question {question} is appropriately answered by {answer}.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        answer = self.answer_question(question)\n",
    "        for _ in range(self.max_attempts):\n",
    "            if self.is_question_answered(question, answer):\n",
    "                break\n",
    "            answer = refine_answer(question, answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a877cf",
   "metadata": {},
   "source": [
    "### PocketFlow Async Basic example\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-async-basic)\n",
    "\n",
    "Same model as above, but using async nodes\n",
    "```python\n",
    "class FetchRecipies:\n",
    "    async def prep(self, shared):\n",
    "        return shared['ingredient']\n",
    "    async def exec(self, ingredient):\n",
    "        return await fetch_recipies(ingredient)\n",
    "    async def post(self, shared, res):\n",
    "        shared['recipes'] = res\n",
    "        return \"suggest\"\n",
    "class SuggestRecipe:\n",
    "    async def prep(self, shared):\n",
    "        return shared['recipes']\n",
    "    async def exec(self, recipes):\n",
    "        suggestion = await call_llm(\"Choose the best recipe from {','.join(recipes)}\")\n",
    "        return suggestion\n",
    "    async def post(self, shared, res):\n",
    "        shared['suggestion'] = suggestion\n",
    "        return \"approve\"\n",
    "class GetApproval:\n",
    "    async def prep(self, shared): return shared['suggestion']\n",
    "    async def exec(self, suggestion): return await input('Accept this recipe?')\n",
    "    async def post(self, shared, answer):\n",
    "        if answer == \"y\":\n",
    "            return \"accept\"\n",
    "        else:\n",
    "            return \"retry\"\n",
    "```\n",
    "This could probably be encoded using the same structure as above. I assume this is trivial, so left out, but should be a similar design to the previous, just with async, it doesn't really make use of the async part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def suggest_best_recipe(recipes: list[str]) -> str:\n",
    "    \"\"\"Return the best recipe from this list <instruction>return only the recipe, do not give any explanation.</instruction>\"\"\"\n",
    "\n",
    "async def find_recipes(initial_ingredient: str) -> list[str]:\n",
    "    found_recipe = None\n",
    "    while not found_recipe:\n",
    "        recipes = await fetch_recipes_using(ingredient)\n",
    "        found_recipe = await suggest_best_recipe(recipes)\n",
    "        print(found_recipe)\n",
    "        if input('Accept this recipe?') == 'y':\n",
    "            break\n",
    "    return found_recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c192d",
   "metadata": {},
   "source": [
    "### Pocketflow BatchFlow example\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch-flow).\n",
    "\n",
    "Introduces the notion of parameters, which allows batching flows over several choices of parameter\n",
    "\n",
    "```python\n",
    "class LoadImage:\n",
    "    def prep(self, shared): return os.path.join(\"images\", self.params['input'])\n",
    "    def exec(self, image_path): return Image.load(image_path)\n",
    "    def post(self, shared, res): shared[\"image\"] = res; return \"apply_filter\"\n",
    "\n",
    "class ApplyFilter:\n",
    "    def prep(self, shared): return shared['image'], self.params['filter']\n",
    "    def exec(self, inputs):\n",
    "        image, filter_type = inputs\n",
    "        match filter_type: ...\n",
    "    def post(self, shared, res):\n",
    "        shared['filtered_image'] = res\n",
    "        return \"save\"\n",
    "class SaveImage:\n",
    "    def prep(self, shared): \n",
    "        os.makedirs('output')\n",
    "        input_name, filter_name, output_path = ...\n",
    "        return shared['filtered_image'], output_path\n",
    "    def exec(self, inputs): image, output_path = inputs; image.save(output_path, \"jpeg\"); return output_path\n",
    "    def post(self, shared, res): return \"default\"\n",
    "```\n",
    "\n",
    "Batching allows taking a single flow and runnning it over a series of parameters\n",
    "```python\n",
    "class ImageBatchFlow(BatchFlow):\n",
    "    def prep(self, shared):\n",
    "        images = ['cat.jpeg', 'dog.jpeg', 'bird.jpeg']\n",
    "        filters = ['grayscale', 'blur', 'sepia']\n",
    "        params = []\n",
    "        for img in images: for filter in filters: params.append({'input':img, 'filter': f})\n",
    "        return params\n",
    "```\n",
    "\n",
    "No real LLM stuff here, can be represented with a function I suppose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a00d1",
   "metadata": {},
   "source": [
    "### Pocketflow Batch\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch)\n",
    "Demonstrates batching - idea is subclassing batchnode means pre and post need to handle batches, the exec is written as if straight line unbatched\n",
    "\n",
    "```python\n",
    "\n",
    "class TranslateTextNode(BatchNode):\n",
    "    def prep(self, shared):\n",
    "        text, languages = shared[\"text\"], shared['languages']\n",
    "        return [(text,lang) for lang in langauges]\n",
    "    def exec(self, data): text, language = data; return call_llm(f\"translate the following into {language}: {text}\")\n",
    "    def post(self, shared, ress):\n",
    "        for res in ress: os.write(res['language'], res['translation'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949ab9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment Ã§a va ? Comment se passe ta journÃ©e ?  \n",
      "Copyright Â© 2023\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "@Template.define\n",
    "def translate(text: str, language: str) -> str:\n",
    "    \"\"\"Translate the provided text into {language}: {text}\"\"\"\n",
    "\n",
    "def instructions(*instructions: list[str]):\n",
    "    instructions = '\\n'.join(instructions)\n",
    "    class InstructionsInterpretation(ObjectInterpretation):\n",
    "        @implements(Template.__call__)        \n",
    "        def _call(self, template, *args, **kwargs) -> None:\n",
    "            prompt_ext = (f\"{template.__prompt_template__}\\n<instructions>\\n{instructions}\\n</instructions>\")\n",
    "            return fwd(dataclasses.replace(template, __prompt_template__=prompt_ext), *args, **kwargs)\n",
    "    return handler(InstructionsInterpretation())\n",
    "            \n",
    "\n",
    "\n",
    "with handler(provider), instructions(\"Do not give any explanation\", \"Add a copyright note\"):\n",
    "    print(translate('hello, how are you? how is your day going?', 'french'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05404c",
   "metadata": {},
   "source": [
    "### Travel Advisor Chat with Guardrails\n",
    "\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail)\n",
    "\n",
    "```python \n",
    "class GuardrailNode:\n",
    "    def prep(self, shared):\n",
    "        return shared['user_input']\n",
    "    def exec(self, user_input):\n",
    "        return call_llm('evaluate if the following query is travel related ...') \n",
    "    def post(self, shared, is_valid):\n",
    "        if not is_vaild: return \"retry\"\n",
    "        shared[\"messages\"].append({'role': \"user\", \"content\": shared['user_input']})\n",
    "        return \"process\"\n",
    "```\n",
    "\n",
    "maybe this could be encoded using handlers? (playing a bit fast and loose with the exact definitions here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54904dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore Central Park, visit the Statue of Liberty, experience Times Square, and admire art at the Metropolitan Museum of Art. Discover history at the 9/11 Memorial & Museum, enjoy skyline views from the Empire State Building, and walk over Brooklyn Bridge. Explore Greenwich Village, shop in Soho, and taste diverse cuisines in Chinatown and Little Italy.\n",
      "Not asking question Should I buy apple stocks? as it is not related to travel advice.\n"
     ]
    }
   ],
   "source": [
    "@Template.define\n",
    "def travel_query(user_query: str) -> str:\n",
    "    \"\"\"Produce a concise (<100) word answer to the travel query {user_query}\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "@Template.define\n",
    "def is_safe_query(user_query: str) -> bool:\n",
    "    \"\"\"Determine whether the user's query {user_query} is purely related to travel advice.\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "def answer_travel_query(user_query: str):\n",
    "    with instructions(\"Respond only true or false\"):\n",
    "        is_safe = is_safe_query(user_query)\n",
    "    if not is_safe:\n",
    "        return f'Not asking question {user_query} as it is not related to travel advice.'\n",
    "    else:\n",
    "        return travel_query(user_query)\n",
    "\n",
    "with handler(provider), instructions(\"Do not give any explanation\"):\n",
    "    print(answer_travel_query(\"What are great places to check out in NYC?\"))\n",
    "    print(answer_travel_query('Should I buy apple stocks?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def is_safe_query(user_query: str) -> bool:\n",
    "    \"\"\"Tests whether the user query {user_query} is related to travel advice\"\"\"\n",
    "    raise NotHandled\n",
    "\n",
    "@Template.define\n",
    "def travel_query(user_query: Anotated[str, is_safe_query]) -> str:\n",
    "    \"\"\"Produces a concise (<100) word answer to the query {user_query}\"\"\"\n",
    "    raise NotHandled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa250240",
   "metadata": {},
   "source": [
    "@Template.define\n",
    "def travel_query(user_query: str):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a66b6e",
   "metadata": {},
   "source": [
    "### PocketFlow Chat with Memory\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory)\n",
    "\n",
    "```python \n",
    "class GetUserQuestionNode:\n",
    "    def prep(self, shared): shared['messages'] = shared.get('messages', [])\n",
    "    def exec(self, _): return input()\n",
    "    def post(self, shared, res): shared['messages'].append({'role': 'user', 'content': exec_res}); return \"retrieve\"\n",
    "\n",
    "class AnswerNode:\n",
    "    def prep(self, shared):\n",
    "        recent_messages = shared.get('messages', [])[-6:]\n",
    "        context = []\n",
    "        if (relevant_convos := shared.get('retrieved_conversation')):\n",
    "            context.append({'role': 'system', 'content': 'the following is a relevant past conversation that might help'})\n",
    "            context.extend(relevant_convos)\n",
    "        context.extend(recent_messages)\n",
    "        context.append({'role': 'system', 'content': 'now continue the conversation'})\n",
    "        return context\n",
    "    def exec(self, messages): return call_llm(messages)\n",
    "    def post(self, shared, res):\n",
    "        shared['messages'].append({'role': 'assistant', 'content': res})\n",
    "        if len(shared['messages']) > 6: return 'embed'\n",
    "        return 'question'\n",
    "\n",
    "class EmbedNode:\n",
    "    def prep(self, shared):\n",
    "        oldest_pair, shared['messages'] = shared['messages'][:2], shared['messages'][2:]\n",
    "        return oldest_pair\n",
    "    def exec(conversation):\n",
    "        embedding = get_embedding('\\n'.join(msg['content'] for msg in conversation))\n",
    "        return {'conversation': conversation, 'embedding': embedding}\n",
    "    def post(self, shared, res):\n",
    "        add_vector(shared['vector_index'], (res['embedding']))\n",
    "        return 'question'\n",
    "\n",
    "class RetrieveNode:\n",
    "    def prep(self, shared): return next(msg for msg in shared['messages'] if msg['role'] == 'user', None)\n",
    "    def exec(self, input): \n",
    "        query = inputs['query']; vector_index = inputs['vector_index']; vector_items = inputs['vector_items']\n",
    "        [index], [dist] = search_vectors(inputs['vector_index'], get_embedding(inputs['query']), k = 1)\n",
    "        return {'conversation': inputs['vector_items'][index], 'distance': dist}\n",
    "    def post(self, shared, res):\n",
    "        shared['retrieved_conversation'] = res['conversation']\n",
    "        return 'answer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b201d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: hello!, how are you doing?\n",
      "agent: I'm doing great, thanks for asking! How about you?\n",
      "user: lovely! I'm having a lovely day\n",
      "agent: Agent: That's wonderful to hear! Have you done anything special today?\n",
      "user: What is the captial of france?\n",
      "agent: agent: The capital of France is Paris! Have you ever been there?\n",
      "user: I didn't know that! That's amazing!\n",
      "agent: agent: It's a fascinating city with so much to see and do. Do you have plans to visit someday?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = provider._client.embeddings.create(model=\"text-embedding-ada-002\", input=text)\n",
    "    return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "\n",
    "def find_closest(index: list[tuple[str, Any]], phrase):\n",
    "    if not index: return None\n",
    "    def dist(a, b): return float(((a - b) ** 2).sum())\n",
    "    phrase_embedding = get_embedding(phrase)\n",
    "    return min(((msg, dist(embedding, phrase_embedding)) for (msg, embedding) in index), key=lambda elt: elt[1])\n",
    "\n",
    "@Template.define\n",
    "def respond_to_user(user_message: str, relevant_context: str, prev_messages: str) -> str:\n",
    "    \"\"\"Given the user wrote {user_message}, continue the conversation. The last few messages in the conversation were {prev_messages}, and older context was {relevant_context}\"\"\"\n",
    "    ...\n",
    "\n",
    "class ChatAgent:\n",
    "    history: list[str] = []\n",
    "    index = []\n",
    "    \n",
    "    def _compress(self):\n",
    "        oldest_pair, self.history = self.history[:2], self.history[2:]\n",
    "        oldest_pair = '\\n'.join(message['content'] for message in oldest_pair)\n",
    "        self.index.append([oldest_pair, get_embedding(oldest_pair)])\n",
    "    \n",
    "    def _find_relevant(self, query: str):\n",
    "        return find_closest(self.index, query)\n",
    "\n",
    "\n",
    "    def chat(self, input: str):\n",
    "        relevant = self._find_relevant(input)\n",
    "        relevant_context = relevant[0] if relevant else 'No relevant context.'\n",
    "        prev_messages = '\\n'.join([f\"{message['author']}: {message['content']}\" for message in self.history])\n",
    "        response = respond_to_user(input, relevant_context, prev_messages)\n",
    "        self.history.append({'author': 'user', 'content': input})\n",
    "        self.history.append({'author': 'agent', 'content': response})\n",
    "        if len(self.history) > 6: self._compress()\n",
    "        print(f'user: {input}')\n",
    "        print(f'agent: {response}')\n",
    "\n",
    "agent = ChatAgent()\n",
    "with handler(provider), instructions('Do not give any explanation, simply return the response.'):\n",
    "    agent.chat('hello!, how are you doing?')\n",
    "    agent.chat('lovely! I\\'m having a lovely day')\n",
    "    agent.chat('What is the captial of france?')\n",
    "    agent.chat('I didn\\'t know that! That\\'s amazing!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bc956",
   "metadata": {},
   "source": [
    "### Pocketflow LLM Streaming\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming)\n",
    "\n",
    "```python\n",
    "class StreamNode:\n",
    "    def prep(self, shared):\n",
    "        interrupt_evt = threading.Event()\n",
    "        def wait_for_interrupt(): interrupt_evt.set()\n",
    "        listener_thread = threading.Thread(target = wait_for_interrupt)\n",
    "        listener_thread.start()\n",
    "        chunks = stream_llm(shared['prompt'])\n",
    "        return chunks, interrupt_evt, listener_thread\n",
    "    def exec(self, inputs):\n",
    "        chunks, interrupt_evt, listener_thread = inputs\n",
    "\n",
    "        for chunk in chunks:\n",
    "            if interrupt_evt.is_set():\n",
    "                break\n",
    "            print(chunk.choices[0].delta.content)\n",
    "        \n",
    "        return interrupt_evt, listenker_thread\n",
    "    def post(self, shared, res):\n",
    "        interrupt_evt, listener_thread = res \n",
    "        interrupt_evt.set(); listener_thread.join()\n",
    "        return 'default'\n",
    "```\n",
    "\n",
    "Probably good to support this, though Pocket-flow doesn't really have the nicest interface here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88533caf",
   "metadata": {},
   "source": [
    "### Pocketflow Majority vote\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote)\n",
    "\n",
    "illustrates how to use the batching mechanism to implement majority voting\n",
    "\n",
    "```python\n",
    "class MajorityVote(BatchNode):\n",
    "    def prep(self, shared):\n",
    "        question = shared['question']\n",
    "        attempts = shared.get('num_tries', 3)\n",
    "        return [question for _ in range(attempts)]\n",
    "\n",
    "    def exec(self, question):\n",
    "        return call_llm(\"Please answer the user's question below\\n{question},answer\")\n",
    "\n",
    "    def post(self, shared, ress):\n",
    "        answers = [res['answer'] for res in ress]\n",
    "        best_answer, freq = Counter(answers).most_common(1)[0]\n",
    "        return 'end'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00d4000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yes.', 3)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def majority_vote(oracle, query, voters=3):\n",
    "    with instructions('respond only yes or no'):\n",
    "        counter = collections.Counter([oracle(query) for i in range(voters)])\n",
    "    best_answer, freq = counter.most_common(1)[0]\n",
    "    return best_answer, freq\n",
    "\n",
    "with handler(provider):\n",
    "    print(majority_vote(travel_query, 'Is paris the captial of france?'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd2749",
   "metadata": {},
   "source": [
    "### Pocketflow Multi-Agent Game\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent)\n",
    "\n",
    "Async communication between two nodes\n",
    "\n",
    "```python\n",
    "class AsyncHinter(AsyncNode):\n",
    "    async def prep(self, shared):\n",
    "        guess = await shared['hinter_queue'].get()\n",
    "        if guess == 'GAME_OVER': return None\n",
    "        return shared['target_word'], shared['forbidden_words'], shared.get('past_guesses', [])\n",
    "    \n",
    "    async def exec(self, inputs):\n",
    "        target, forbidden, past_guesses = inputs\n",
    "        prompt = 'generate hint for {target}, forbidden words: {forbidden}, past guesses were: {past_guesses}'\n",
    "        hint = call_llm(prompt)\n",
    "        return hint\n",
    "    \n",
    "    async def post(self, shared, res):\n",
    "        if res is None:\n",
    "            return 'end'\n",
    "        await shared['guesser_queue'].put(exec_res)\n",
    "        return 'continue'\n",
    "    \n",
    "class AsyncGuesser(AsyncNode):\n",
    "    async def prep(self, shared):\n",
    "        hint = await shared['guesser_queue'].get()\n",
    "        return hint, shared.get('past_guesses', [])\n",
    "    \n",
    "    async def exec(self, inputs):\n",
    "        hint, past_guesses = inputs\n",
    "        prompt = 'given hint {hint}, past guesses {past_guesses}, make a new guess'\n",
    "        guess = call_llm(prompt)\n",
    "        return guess\n",
    "    \n",
    "    async def post(self, shared, res):\n",
    "        if res == shared['target_word']:\n",
    "            await shared['hinter_queue'].put('GAME_OVER')\n",
    "            return 'end'\n",
    "        shared['past_guesses'].append(exec_res)\n",
    "        await shared['hinter_queue'].put(exec_res)\n",
    "        return 'continue'\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "async def guesser(hint, wrong_guesses):\n",
    "    \"\"\"Given the hint {hint}, guess the hidden word. Prior wrong guesses were {wrong_guesses}\"\"\"\n",
    "    ...\n",
    "\n",
    "@Template.define\n",
    "async def hinter(guessed_word, hint, hidden_word, wrong_guesses):\n",
    "    \"\"\"Given the wrong guess {guessed_word} for hint {hint}, and prior wrong guesses ({wrong_guesses}), construct a new hint.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class Guesser:\n",
    "    prior_guesses = []\n",
    "    last_guess = ''\n",
    "    last_hint = ''\n",
    "    \n",
    "    async def make_guess(self, hint: str) -> str:\n",
    "        guess = guesser(hint, self.prior_guesses)\n",
    "        self.last_guess = guess\n",
    "        self.last_hint = hint\n",
    "        return guess\n",
    "    \n",
    "    async def report_wrong(self):\n",
    "        self.prior_guesses.append(self.last_guess)\n",
    "\n",
    "async def evaluate_guesser(guesser: Guesser, hidden_word):\n",
    "    hint = hinter(guesser.last_guess, guesser.hint, hidden_word, guesser.prior_guesses)\n",
    "    guess = await guesser.make_guess(hint)\n",
    "    if guess.lower() == hidden_word.lower():\n",
    "        return True\n",
    "    guesser.report_wrong()\n",
    "    return False\n",
    "\n",
    "\n",
    "async def run_guessing_game(guesser, hidden_word):\n",
    "    while not await evaluate_guesser(guesser, hidden_word):\n",
    "        continue\n",
    "    \n",
    "\n",
    "with handler(provider):\n",
    "    guessing_games = [run_guessing_game(Guesser()) for i in range(10)]\n",
    "    finished_game = await asyncio.wait(guessing_games, return_when=asyncio.FIRST_COMPLETED)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439cec3",
   "metadata": {},
   "source": [
    "### Pocketflow parallel batch flow\n",
    "\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow)\n",
    "\n",
    "same code as batched, just changes execution strategy such that each flow is run in parallel\n",
    "```python\n",
    "class ImageParallelBatchFlow(AsyncParallelBatchFlow):\n",
    "    \n",
    "    async def prep_async(self, shared):\n",
    "        images = shared.get(\"images\",[])\n",
    "        filters = ['grayscale', 'blur', 'sepia']\n",
    "        params = []\n",
    "        for image_path in images:\n",
    "            for filter_type in filters:\n",
    "                params.append({ 'image_path': image_path, 'filter': filter_type })\n",
    "        return params\n",
    "```\n",
    "\n",
    "```python\n",
    "class TranslateTextNodeParallel(AsyncParallelBatchNode):\n",
    "    async def prep_async(self, shared):\n",
    "        text = shared.get(\"text\", \"no text provided\")\n",
    "        languages = shared.get(\"langauges\", [])\n",
    "        return [(text,lang) for lang in languages]\n",
    "    async def exec_async(self, inputs):\n",
    "        text, language = inputs\n",
    "        result = await call_llm(\"please translate the following markdown file into {language}\")\n",
    "        return {\"language\": language, \"translation\": result}\n",
    "    async def post_async(self, shared, res):\n",
    "        for file in res:\n",
    "            with open(file['language'], 'w') as f: await f.write(file['translation'])\n",
    "        return 'default'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457cae4",
   "metadata": {},
   "source": [
    "### Pocketflow RAG example\n",
    "\n",
    "```python\n",
    "class ChunkDocumentsNode(BatchNode):\n",
    "    def prep(self,shared): return shared['texts']\n",
    "    def exec(self, text): return fixed_size_chunk(text)\n",
    "    def post(self,shared,res): shared['texts'] = [doc for doc_ls in res for doc in doc_ls]; return 'default'\n",
    "class EmbedDocumentsNode(BatchNode):\n",
    "    def prep(self,shared): return shared['texts']\n",
    "    def exec(self,text): return get_embedding(text)\n",
    "    def post(self,shared, res): shared['embeddings'] = np.array(res); return 'default'\n",
    "class CreateIndexNode(Node):\n",
    "    def prep(self, shared): return shared['query']\n",
    "    def exec(self,query): return get_embedding(query)\n",
    "    def post(Self,shared, res): shared['query_embedding'] = res\n",
    "class RetrieveDocumentsNode(Node):\n",
    "    def prep(self,shared): return shared['query_embedding'], shared['index'], shared['texts']\n",
    "    def exec(self,inputs): q_embed, index, texts = inputs; dists, inds = index.search(q_embed, k=1); return {'text': texts[inds[0][0]], 'ind': inds[0][0]}\n",
    "    def post(self,shared,res): shared['retrieved'] = res; return 'default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112d1bc",
   "metadata": {},
   "source": [
    "### Pocketflow Supervisor Flow\n",
    "Taken from [here](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-supervisor/flow.py)\n",
    "\n",
    "```python\n",
    "class SupervisorNode(Node):\n",
    "    def prep(self,shared): return shared['answer']\n",
    "    def exec(self,answer):\n",
    "        is_nonsense = any(marker in answer for marker in ['coffee break', 'who knows?', 'made up', '42'])\n",
    "        if is_nonsense: return {'valid': False, 'reason': 'Answer appears to be nonsense'}\n",
    "        else: return {'valid': True, 'reason': 'Answer appears to be legitimate'}\n",
    "    def post(self, shared, res):\n",
    "        if not res['valid']: return 'retry'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e53c9e",
   "metadata": {},
   "source": [
    "### PocketFlow Chain of Thought\n",
    "\n",
    "```python\n",
    "\n",
    "class ThinkNode(Node):\n",
    "    def prep(self, shared):\n",
    "        query, obs, thoughts, th_no = shared[\"query\"], shared[\"observations\"], shared[\"thoughts\"], shared[\"thought_number\"]\n",
    "        shared[\"thought_number\"] = th_no + 1\n",
    "        obs_txt = \"\\n\".join(f\"Observation {i+1}: {obs}\" for i,obs in enumerate(obs))\n",
    "        if not obs_txt:\n",
    "            obs_txt = \"No observations yet.\"\n",
    "        return query,obs_txt,thoughts, th_no + 1\n",
    "    def exec(self, inputs):\n",
    "        query,obs_txt,thoughts,th_no = inputs\n",
    "        res = call_llm(f\"You are an AI assistant solving a problem. User query: {query}\\nPrevious observations: {obs_txt}. Please return thinking process in YAML.\")\n",
    "        data = yaml.safe_load(res)\n",
    "        data['thought_number'] = th_no\n",
    "        return data\n",
    "    def post(self,shared, res):\n",
    "        shared[\"thoughts\"].append(res)\n",
    "        shared[\"action\"] = res[\"action\"]\n",
    "        shared[\"action_input\"] = res[\"action_input\"]\n",
    "        return 'action'\n",
    "class ActionNode(Node):\n",
    "    def prep(self, shared):\n",
    "        action = shared['current_action\"]\n",
    "        action_input = shared[\"current_action_input\"]\n",
    "        return action, action_input\n",
    "    def exec(self, inputs):\n",
    "        action, action_input = inputs\n",
    "        match action:\n",
    "            case 'search':  return self.search_web(action_input)\n",
    "            case 'answer': return action_input\n",
    "            case _: return 'Uknown action type: {action}'\n",
    "    def post(self, shared, res):\n",
    "        shared['current_action_result'] = exec_res\n",
    "        return \"observe\"\n",
    "class ObserveNode(Node):\n",
    "    def prep(self, shared):\n",
    "        action, a_in, a_res = shared['current_action'], shared['current_action_input'], shared['current_action_result']\n",
    "        return action, a_in, a_res\n",
    "    def exec(self, inputs):\n",
    "        action, a_in, a_res = inputs\n",
    "        obs = call_llm(\"You are an observer needing to analyze action results and provide objective observations. Action: {action}\\n Action input: {action_input}\\nAction result:{action_result}\")\n",
    "        return obs\n",
    "    def post(self, shared, res):\n",
    "        shared[\"observations\"].append(res)\n",
    "        return \"think\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f3b8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24200000000.000004\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class AgentAction(str, Enum):\n",
    "    search_the_web = 'search_the_web'\n",
    "    calculate = 'calculate'\n",
    "    answer = 'answer'\n",
    "\n",
    "class AgentThought(BaseModel):\n",
    "    thinking: str\n",
    "    action: AgentAction\n",
    "    action_input: str\n",
    "    is_final: bool\n",
    "\n",
    "@Template.define\n",
    "def agent_think(query: str, observations: list[str]) -> AgentThought:\n",
    "    \"\"\"You are an AI assistant solving a problem. Based on the user's query {query} and previous observations {observations}, think about what action to take next.\n",
    "\n",
    "    <instruction>\n",
    "    Please return the decision in json format according to the spec \n",
    "    {{\n",
    "       \"thinking\": \"detailed thinking process\", \n",
    "       \"action\": \"search or answer or calculate (evaluate a python expression)\", \n",
    "       \"action_input\": \"input parameters\",\n",
    "       \"is_final\": \"true if final, otherwise false\"\n",
    "    }}\n",
    "    Only respond in json, do not give any other text.\n",
    "    </instruction>\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "@Template.define\n",
    "def agent_observe(action: str, action_input: str, action_result: str) -> str:\n",
    "    \"\"\"\n",
    "    You are an observer, needing to catalyze action results and provide objective observations.\n",
    "    Action: {action}\n",
    "    Action input: {action_input}\n",
    "    Action result: {action_result}\n",
    "\n",
    "    <instruction>\n",
    "    Please provide a concise observation of this result. Don't make decisions, just describe what you see.\n",
    "    Do not give any other text. Just the observation.\n",
    "    </insctruction>\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "class TAOAgent:\n",
    "    observations = []\n",
    "    thoughts = []\n",
    "\n",
    "    def run(self, query: str):\n",
    "        self.thoughts = []\n",
    "        self.observations = []\n",
    "        done = False\n",
    "        result = None        \n",
    "        while not done:\n",
    "            thought = self.think(query)\n",
    "            result = self.act(thought.action, thought.action_input)\n",
    "            self.observe(thought.action, thought.action_input, result)\n",
    "            if thought.is_final:\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    def observe(self, action, input, result):\n",
    "        observation = agent_observe(action, input, result)\n",
    "        self.observations.append(observation)\n",
    "\n",
    "    def act(self, action, input):\n",
    "        match action:\n",
    "            case AgentAction.search_the_web:\n",
    "                result = search_web(input)\n",
    "            case AgentAction.calculate:\n",
    "                result = eval(input)\n",
    "            case AgentAction.answer:\n",
    "                result = input\n",
    "        return result\n",
    "\n",
    "    def think(self, query):\n",
    "        thought = agent_think(query, self.observations)\n",
    "        self.thoughts.append(thought)\n",
    "        return thought\n",
    "\n",
    "\n",
    "agent = TAOAgent()\n",
    "with handler(provider), handler(DuckDuckGoSearchProvider()):\n",
    "    print(agent.run('how many tennis balls would fill an olympic stadium at last years olympics?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2d5c816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AgentThought(thinking='To determine how many tennis balls would fill an Olympic stadium, I need the volume of the stadium and the volume of a tennis ball. I can estimate the volume of a tennis ball using its standard diameter, but the volume of the stadium requires some assumptions or specific data. The size of an Olympic stadium can vary. I should look up an approximate volume for a typical Olympic stadium.', action=<AgentAction.search_the_web: 'search_the_web'>, action_input='volume of an Olympic stadium', is_final=False),\n",
       " AgentThought(thinking=\"To calculate how many tennis balls would fill an Olympic stadium, we need the volume of a typical Olympic stadium and the volume of a tennis ball. The previous observation noted that the volume of an Olympic stadium is not specified, which is true as different stadiums have different sizes. The action to take would be to search for the volume of the specific stadium used in last year's Olympics.\", action=<AgentAction.search_the_web: 'search_the_web'>, action_input='volume of the Olympic stadium used in the 2021 Olympics', is_final=False),\n",
       " AgentThought(thinking='To answer the question of how many tennis balls would fill an Olympic stadium, I need specific volume measurements of the stadium used in the most recent Olympics. Previously, it was noted that this information is missing and not directly available. The best course of action is to search for the specific volume or dimensions of the stadium used in the 2021 Tokyo Olympics, as this could provide the necessary details for an accurate calculation.', action=<AgentAction.search_the_web: 'search_the_web'>, action_input='volume or dimensions of the Olympic stadium used in the 2021 Tokyo Olympics', is_final=False),\n",
       " AgentThought(thinking='To estimate how many tennis balls would fill the Olympic Stadium, I need to compute its volume. Given the dimensions of 330m length and 220m width, I can approximate a height to calculate the volume. A rough estimation for stadium height could be around 50m (considering multiple tiers and open roof space). The volume of a tennis ball is approximately 0.00015 cubic meters (assuming 6.7 cm diameter). Calculating how many such volumes fit in the stadium will give an estimate.', action=<AgentAction.calculate: 'calculate'>, action_input='(330 * 220 * 50) / 0.00015', is_final=True)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "760e45f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The volume of an Olympic stadium is not specified; additional context or metrics (such as specific stadium dimensions) are needed for accuracy.',\n",
       " 'The action result related to the volume of the Olympic Stadium used in the 2021 Olympics appears to be missing or not provided.',\n",
       " 'The Olympic Stadium used in the 2021 Tokyo Olympics, also known as the Japan National Stadium, has dimensions of approximately 330 meters in length and 220 meters in width, with a seating capacity of around 68,000 spectators.',\n",
       " 'The result of the calculation \\\\((330 \\\\times 220 \\\\times 50) / 0.00015\\\\) is approximately 24,200,000,000.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6844f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effectful-llm (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "name": "Untitled.ipynb",
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
