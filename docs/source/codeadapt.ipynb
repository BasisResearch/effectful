{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd821b7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import typing\n",
        "from effectful.handlers.llm.template import Template\n",
        "from effectful.ops.types import NotHandled\n",
        "from typing import Callable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fde56515",
      "metadata": {},
      "outputs": [],
      "source": [
        "@Template.define\n",
        "def generate_paragraph() -> str:\n",
        "    \"\"\"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\n",
        "    \"\"\"\n",
        "    raise NotHandled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "301234cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "@Template.define\n",
        "def codeact(\n",
        "    template_name: str,\n",
        "    args_json: str = \"[]\",\n",
        "    kwargs_json: str = \"{}\",\n",
        ") -> Callable[[], str]:\n",
        "    \"\"\"Generate a code that solve the following problem:\n",
        "    {template_name}\n",
        "    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\n",
        "    DO NOT USE codeadapt tool.\n",
        "    \"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def codeadapt(\n",
        "    template_name: str,\n",
        "    args_json: str = \"[]\",\n",
        "    kwargs_json: str = \"{}\",\n",
        ") -> str:\n",
        "    \"\"\"Reason about the template, uses the codeact tool to generate a code that solve the problem.\n",
        "    The template:\n",
        "    {template_name}\n",
        "    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\n",
        "    \"\"\"\n",
        "    raise NotHandled\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d793813",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "import json\n",
        "\n",
        "from effectful.handlers.llm.completions import (\n",
        "    DecodedToolCall,\n",
        "    Encodable,\n",
        "    LiteLLMProvider,\n",
        "    Message,\n",
        "    RetryLLMHandler,\n",
        "    call_assistant,\n",
        "    call_system,\n",
        "    call_tool,\n",
        "    call_user,\n",
        "    get_message_sequence,\n",
        "    handler,\n",
        "    implements,\n",
        ")\n",
        "from effectful.handlers.llm.evaluation import RestrictedEvalProvider\n",
        "\n",
        "\n",
        "class ToolNotUsedError(Exception):\n",
        "    \"\"\"Exception raised when a tool is not used in the code.\"\"\"\n",
        "\n",
        "    tool_name: str\n",
        "\n",
        "\n",
        "class CodeAdapt(LiteLLMProvider):\n",
        "    def __init__(self, model: str = \"gpt-4o\"):\n",
        "        super().__init__(model=model)\n",
        "\n",
        "    @implements(Template.__apply__)\n",
        "    def _call[**P, T](self, template: Template[P, T], *args: P.args, **kwargs: P.kwargs) -> T:\n",
        "        tool_called = False\n",
        "\n",
        "        # Avoid recursive handling when codeadapt is invoked on a Template argument.\n",
        "        if template is codeadapt and args and isinstance(args[0], Template):\n",
        "            args = (args[0].__name__, *args[1:])\n",
        "\n",
        "        message_sequence = get_message_sequence()\n",
        "        with handler({get_message_sequence: lambda: message_sequence}), handler(\n",
        "            RetryLLMHandler()\n",
        "        ):\n",
        "            # encode arguments\n",
        "            bound_args = inspect.signature(template).bind(*args, **kwargs)\n",
        "            bound_args.apply_defaults()\n",
        "            env = template.__context__.new_child(bound_args.arguments)\n",
        "\n",
        "            # Create response_model with env so tools passed as arguments are available\n",
        "            response_model = Encodable.define(\n",
        "                template.__signature__.return_annotation, env\n",
        "            )\n",
        "\n",
        "            call_system(template)\n",
        "            message: Message = call_user(template.__prompt_template__, env)\n",
        "\n",
        "            # loop based on: https://cookbook.openai.com/examples/reasoning_function_calls\n",
        "            tool_calls: list[DecodedToolCall] = []\n",
        "            result: T | None = None\n",
        "            while message[\"role\"] != \"assistant\" or tool_calls:\n",
        "                print(json.dumps(message_sequence, indent=2))\n",
        "                message, tool_calls, result = call_assistant(\n",
        "                    template.tools, response_model, **self.config\n",
        "                )\n",
        "                for tool_call in tool_calls:\n",
        "                    message = call_tool(tool_call)\n",
        "                    tool_called = True\n",
        "\n",
        "            assert result is not None, (\n",
        "                \"call_assistant did not produce a result nor tool_calls\"\n",
        "            )\n",
        "            assert tool_called, \"No tool was called\"\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "538ced14",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mlitellm.completion('gpt-4o', messages=[{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}], response_format=<class 'effectful.handlers.llm.completions.Response'>, tools=[{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}])\u001b[0m\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m10:21:29 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:3422 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': <class 'effectful.handlers.llm.completions.Response'>, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:3425 - \n",
            "LiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}]}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - Final returned optional params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1048 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "https://api.openai.com/v1/ \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}], 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1121 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D5CVVRDyFclbhT4Bth9KRjAJ4Gbm3\", \"choices\": [{\"finish_reason\": \"tool_calls\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": null, \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": [{\"id\": \"call_jlVeWXkicrogWdNVU9WUIjQQ\", \"function\": {\"arguments\": \"{\\\"template_name\\\":\\\"generate_paragraph\\\",\\\"args_json\\\":\\\"{}\\\", \\\"kwargs_json\\\":\\\"{}\\\"}\", \"name\": \"codeact\"}, \"type\": \"function\"}]}}], \"created\": 1770132089, \"model\": \"gpt-4o-2024-08-06\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_fa7f5b168b\", \"usage\": {\"completion_tokens\": 27, \"prompt_tokens\": 208, \"total_tokens\": 235, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
            "\n",
            "\n",
            "\u001b[92m10:21:30 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1801 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0007900000000000001\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0007900000000000001\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mlitellm.completion('gpt-4o', messages=[{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}], response_format=<class 'effectful.handlers.llm.completions.Response'>, tools=[{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}])\u001b[0m\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:30 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3422 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': <class 'effectful.handlers.llm.completions.Response'>, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3425 - \n",
            "LiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}]}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - Final returned optional params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1048 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "https://api.openai.com/v1/ \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}], 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1121 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D5CVWC0noOQb9RbOWjkQau6UgJjWJ\", \"choices\": [{\"finish_reason\": \"tool_calls\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": null, \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": [{\"id\": \"call_pETtdSVT5nqSHg8v7RUQi1hh\", \"function\": {\"arguments\": \"{}\", \"name\": \"generate_paragraph\"}, \"type\": \"function\"}]}}], \"created\": 1770132090, \"model\": \"gpt-4o-2024-08-06\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_fa7f5b168b\", \"usage\": {\"completion_tokens\": 11, \"prompt_tokens\": 358, \"total_tokens\": 369, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
            "\n",
            "\n",
            "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1801 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.001005\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.001005\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mlitellm.completion('gpt-4o', messages=[{'role': 'user', 'content': [{'type': 'text', 'text': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\"}], 'id': '04677efe-0114-11f1-a9bf-b15022a67ef9'}], response_format=<class 'effectful.handlers.llm.completions.Response'>, tools=[{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}])\u001b[0m\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3422 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': <class 'effectful.handlers.llm.completions.Response'>, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\"}], 'id': '04677efe-0114-11f1-a9bf-b15022a67ef9'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3425 - \n",
            "LiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}]}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - Final returned optional params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1048 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "https://api.openai.com/v1/ \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\"}], 'id': '04677efe-0114-11f1-a9bf-b15022a67ef9'}], 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1121 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D5CVX1PSibnBKfARHObDQrFBqMnDR\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"{\\\"value\\\":\\\"The park was a serene sanctuary where she enjoyed her daily walk. As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. She often found solace in a world so lively yet always searching for another. The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\"}\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1770132091, \"model\": \"gpt-4o-2024-08-06\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_35ce66ad6c\", \"usage\": {\"completion_tokens\": 80, \"prompt_tokens\": 235, \"total_tokens\": 315, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
            "\n",
            "\n",
            "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1801 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0013875\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0013875\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mlitellm.completion('gpt-4o', messages=[{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'generate_paragraph'}, 'id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '046775da-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': 'The park was a serene sanctuary where she enjoyed her daily walk. As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. She often found solace in a world so lively yet always searching for another. The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.'}], 'tool_call_id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'id': '05f2934e-0114-11f1-a9bf-b15022a67ef9'}], response_format=<class 'effectful.handlers.llm.completions.Response'>, tools=[{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}])\u001b[0m\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3422 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': <class 'effectful.handlers.llm.completions.Response'>, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'generate_paragraph'}, 'id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'type': 'function'}], 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '046775da-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': 'The park was a serene sanctuary where she enjoyed her daily walk. As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. She often found solace in a world so lively yet always searching for another. The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.'}], 'tool_call_id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'id': '05f2934e-0114-11f1-a9bf-b15022a67ef9'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3425 - \n",
            "LiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}]}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - Final returned optional params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1048 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "https://api.openai.com/v1/ \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Generate a code that solve the following problem:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n'}], 'id': '042cb97c-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'generate_paragraph'}, 'id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'type': 'function'}], 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '046775da-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': 'The park was a serene sanctuary where she enjoyed her daily walk. As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. She often found solace in a world so lively yet always searching for another. The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.'}], 'tool_call_id': 'call_pETtdSVT5nqSHg8v7RUQi1hh', 'id': '05f2934e-0114-11f1-a9bf-b15022a67ef9'}], 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'$defs': {'TypedSynthesizedFunction': {'description': 'Given the specification above, generate a Python function satisfying the following specification and type signature.\\n\\n<signature>Callable[[], str]</signature>\\n\\n<instructions>\\n1. Produce one block of Python code.\\n2. The function MUST have type annotations for all parameters and the return type.\\n3. The function definition must be the LAST statement - do not add any code after it.\\n4. Do not include usage examples or function calls.\\n</instructions>', 'properties': {'module_code': {'description': 'Complete Python module code (no imports needed)', 'title': 'Module Code', 'type': 'string'}}, 'required': ['module_code'], 'title': 'TypedSynthesizedFunction', 'type': 'object', 'additionalProperties': False}}, 'additionalProperties': False, 'properties': {'value': {'$ref': '#/$defs/TypedSynthesizedFunction'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeadapt', 'description': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1121 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D5CVZm0orCsZKaOAmSRD8g0pDu2Sr\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"{\\\"value\\\":{\\\"module_code\\\":\\\"from typing import Callable\\\\n\\\\ndef generate_paragraph() -> str:\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    Generates a paragraph with exactly 4 sentences ending with specified words.\\\\n    Returns:\\\\n        str: A paragraph with sentences ending in 'walk', 'tumbling', 'another', and 'lunatic'.\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    return (\\\\\\\"The park was a serene sanctuary where she enjoyed her daily walk. \\\\\\\"\\\\n            \\\\\\\"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \\\\\\\"\\\\n            \\\\\\\"She often found solace in a world so lively yet always searching for another. \\\\\\\"\\\\n            \\\\\\\"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\\\\\")\\\\n\\\"}}\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1770132093, \"model\": \"gpt-4o-2024-08-06\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_fa7f5b168b\", \"usage\": {\"completion_tokens\": 175, \"prompt_tokens\": 450, \"total_tokens\": 625, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
            "\n",
            "\n",
            "\u001b[92m10:21:36 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1801 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.002875\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.002875\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \u001b[92mlitellm.completion('gpt-4o', messages=[{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{\"template_name\":\"generate_paragraph\",\"args_json\":\"{}\", \"kwargs_json\":\"{}\"}', 'name': 'codeact'}, 'id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '042ca1d0-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': '{\"module_code\":\"def generate_paragraph() -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    Generates a paragraph with exactly 4 sentences ending with specified words.\\\\n    Returns:\\\\n        str: A paragraph with sentences ending in \\'walk\\', \\'tumbling\\', \\'another\\', and \\'lunatic\\'.\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return (\\\\\"The park was a serene sanctuary where she enjoyed her daily walk. \\\\\"\\\\n            \\\\\"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \\\\\"\\\\n            \\\\\"She often found solace in a world so lively yet always searching for another. \\\\\"\\\\n            \\\\\"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\\\")\\\\n\"}'}], 'tool_call_id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'id': '07ab109e-0114-11f1-a9bf-b15022a67ef9'}], response_format=<class 'effectful.handlers.llm.completions.Response'>, tools=[{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}])\u001b[0m\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - \n",
            "\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:36 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:3422 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': <class 'effectful.handlers.llm.completions.Response'>, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{\"template_name\":\"generate_paragraph\",\"args_json\":\"{}\", \"kwargs_json\":\"{}\"}', 'name': 'codeact'}, 'id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'type': 'function'}], 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '042ca1d0-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': '{\"module_code\":\"def generate_paragraph() -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    Generates a paragraph with exactly 4 sentences ending with specified words.\\\\n    Returns:\\\\n        str: A paragraph with sentences ending in \\'walk\\', \\'tumbling\\', \\'another\\', and \\'lunatic\\'.\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return (\\\\\"The park was a serene sanctuary where she enjoyed her daily walk. \\\\\"\\\\n            \\\\\"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \\\\\"\\\\n            \\\\\"She often found solace in a world so lively yet always searching for another. \\\\\"\\\\n            \\\\\"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\\\")\\\\n\"}'}], 'tool_call_id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'id': '07ab109e-0114-11f1-a9bf-b15022a67ef9'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:3425 - \n",
            "LiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}]}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:381 - Final returned optional params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:509 - self.optional_params: {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1048 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "https://api.openai.com/v1/ \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Reason about the template, uses the codeact tool to generate a code that solve the problem.\\n    The template:\\n    generate_paragraph\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n'}], 'id': '03acfb74-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{\"template_name\":\"generate_paragraph\",\"args_json\":\"{}\", \"kwargs_json\":\"{}\"}', 'name': 'codeact'}, 'id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'type': 'function'}], 'provider_specific_fields': {'refusal': None}, 'annotations': [], 'id': '042ca1d0-0114-11f1-a9bf-b15022a67ef9'}, {'role': 'tool', 'content': [{'type': 'text', 'text': '{\"module_code\":\"def generate_paragraph() -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    Generates a paragraph with exactly 4 sentences ending with specified words.\\\\n    Returns:\\\\n        str: A paragraph with sentences ending in \\'walk\\', \\'tumbling\\', \\'another\\', and \\'lunatic\\'.\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return (\\\\\"The park was a serene sanctuary where she enjoyed her daily walk. \\\\\"\\\\n            \\\\\"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \\\\\"\\\\n            \\\\\"She often found solace in a world so lively yet always searching for another. \\\\\"\\\\n            \\\\\"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\\\")\\\\n\"}'}], 'tool_call_id': 'call_jlVeWXkicrogWdNVU9WUIjQQ', 'id': '07ab109e-0114-11f1-a9bf-b15022a67ef9'}], 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'additionalProperties': False, 'properties': {'value': {'title': 'Value', 'type': 'string'}}, 'required': ['value'], 'title': 'Response', 'type': 'object'}, 'name': 'Response', 'strict': True}}, 'tools': [{'type': 'function', 'function': {'name': 'generate_paragraph', 'description': \"Please generate a paragraph: with exactly 4 sentences ending with 'walk', 'tumbling', 'another', and 'lunatic'.\\n\", 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}, {'type': 'function', 'function': {'name': 'codeact', 'description': 'Generate a code that solve the following problem:\\n    {template_name}\\n    Args/kwargs are provided as JSON strings (args_json, kwargs_json).\\n    DO NOT USE codeadapt tool.\\n', 'parameters': {'additionalProperties': False, 'properties': {'template_name': {'title': 'Template Name', 'type': 'string'}, 'args_json': {'title': 'Args Json', 'type': 'string'}, 'kwargs_json': {'title': 'Kwargs Json', 'type': 'string'}}, 'required': ['template_name', 'args_json', 'kwargs_json'], 'title': 'Params', 'type': 'object'}, 'strict': True}}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1121 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D5CVcPmcsKjAlhTbMMHeCljJ0WRaF\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"{\\\"value\\\":\\\"Here is the generated code to solve the problem of creating a paragraph with sentences ending in specific words:\\\\n\\\\n```python\\\\ndef generate_paragraph() -> str:\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    Generates a paragraph with exactly 4 sentences ending with specified words.\\\\n    Returns:\\\\n        str: A paragraph with sentences ending in 'walk', 'tumbling', 'another', and 'lunatic'.\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    return (\\\\\\\"The park was a serene sanctuary where she enjoyed her daily walk. \\\\\\\"\\\\n            \\\\\\\"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \\\\\\\"\\\\n            \\\\\\\"She often found solace in a world so lively yet always searching for another. \\\\\\\"\\\\n            \\\\\\\"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\\\\\\\")\\\\n```\\\\n\\\\nThis function will generate a paragraph composed of four sentences, each ending with the specified words: \\\\\\\"walk\\\\\\\", \\\\\\\"tumbling\\\\\\\", \\\\\\\"another\\\\\\\", and \\\\\\\"lunatic\\\\\\\".\\\"}\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1770132096, \"model\": \"gpt-4o-2024-08-06\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_fa7f5b168b\", \"usage\": {\"completion_tokens\": 229, \"prompt_tokens\": 406, \"total_tokens\": 635, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
            "\n",
            "\n",
            "\u001b[92m10:21:39 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1801 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:851 - selected model name for cost calculation: openai/gpt-4o-2024-08-06\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0033050000000000006\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the generated code to solve the problem of creating a paragraph with sentences ending in specific words:\n",
            "\n",
            "```python\n",
            "def generate_paragraph() -> str:\n",
            "    \"\"\"\n",
            "    Generates a paragraph with exactly 4 sentences ending with specified words.\n",
            "    Returns:\n",
            "        str: A paragraph with sentences ending in 'walk', 'tumbling', 'another', and 'lunatic'.\n",
            "    \"\"\"\n",
            "    return (\"The park was a serene sanctuary where she enjoyed her daily walk. \"\n",
            "            \"As she played on the grassy slopes, joyful laughter filled the air as she went tumbling. \"\n",
            "            \"She often found solace in a world so lively yet always searching for another. \"\n",
            "            \"The night, however, revealed a different side of the city, where the streets whispered the tales of a passing lunatic.\")\n",
            "```\n",
            "\n",
            "This function will generate a paragraph composed of four sentences, each ending with the specified words: \"walk\", \"tumbling\", \"another\", and \"lunatic\".\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1402 - response_cost: 0.0033050000000000006\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-2024-08-06', 'combined_model_name': 'openai/gpt-4o-2024-08-06', 'stripped_model_name': 'gpt-4o-2024-08-06', 'combined_stripped_model_name': 'openai/gpt-4o-2024-08-06', 'custom_llm_provider': 'openai'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m10:21:39 - LiteLLM:DEBUG\u001b[0m: utils.py:5171 - model_info: {'key': 'gpt-4o-2024-08-06', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 2.5e-06, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 1.25e-06, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 1.25e-06, 'output_cost_per_token_batches': 5e-06, 'output_cost_per_token': 1e-05, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}\n"
          ]
        }
      ],
      "source": [
        "from effectful.handlers.llm.evaluation import UnsafeEvalProvider\n",
        "import litellm\n",
        "\n",
        "litellm._turn_on_debug()\n",
        "\n",
        "code_adapt = CodeAdapt(model=\"gpt-4o\")\n",
        "with handler(LiteLLMProvider(model=\"gpt-4o\")), handler(UnsafeEvalProvider()):\n",
        "    res = codeadapt(\"generate_paragraph\")\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2520fbf8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
