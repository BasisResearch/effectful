{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5aaf649f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import functools\n",
        "\n",
        "import dotenv\n",
        "import litellm\n",
        "import pydantic\n",
        "from litellm.caching.caching import Cache\n",
        "from pydantic import field_validator\n",
        "from pydantic_core import PydanticCustomError\n",
        "\n",
        "from effectful.handlers.llm import Template, Tool\n",
        "from effectful.handlers.llm.completions import (\n",
        "    LiteLLMProvider,\n",
        "    RetryLLMHandler,\n",
        "    call_assistant,\n",
        "    call_tool,\n",
        "    get_message_sequence,\n",
        ")\n",
        "from effectful.ops.semantics import NotHandled, fwd, handler\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "provider = LiteLLMProvider()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9e861b",
      "metadata": {},
      "source": [
        "## Interface\n",
        "\n",
        "The `effectful.handlers.llm` module provides a simplified LLM interface that uses algebraic effects for modularity. The module interface consists of:\n",
        "\n",
        "- A decorator `Template.define` which creates a prompt template from a callable. A template is an LLM-implemented function whose behavior is specified by a template string. When a template is called, an LLM is invoked to produce the specified behavior.\n",
        "- A decorator `Tool.define` which exposes Python callables as tools that templates can call. Tool signatures and docstrings define the schema passed to the model.\n",
        "- Structured output handling via `Encodable` (used internally by templates and tool calls) to serialize/deserialize Python types.\n",
        "- LLM providers such as `LiteLLMProvider`, and reliability helpers like `RetryLLMHandler` and `ReplayLiteLLMProvider`, which can be composed with `handler(...)` to control execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c639d3",
      "metadata": {},
      "source": [
        "## Prompt Templates\n",
        "\n",
        "This template function writes (bad) poetry on a given theme. While difficult to implement in Python, an LLM can provide a reasonable implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e832675",
      "metadata": {},
      "outputs": [],
      "source": [
        "@Template.define\n",
        "def limerick(theme: str) -> str:\n",
        "    \"\"\"Write a limerick on the theme of {theme}. Do not use any tools.\"\"\"\n",
        "    raise NotHandled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ca6919",
      "metadata": {},
      "source": [
        "If we call the template with a provider interpretation installed, we get reasonable behavior. The LLM is nondeterministic by default, so calling the template twice with the same arguments gives us different results.\n",
        "\n",
        "Templates are regular callables, so can be converted to operations with `defop` if we want to override the LLM implementation in some cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "634f6533",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the sea, a sly fish did dwell,\n",
            "Who loved spinning tales it would tell.  \n",
            "With scales that would gleam,  \n",
            "It chased the big dream,  \n",
            "To swim where the free dolphins swell.\n",
            "----------------------------------------\n",
            "In the ocean, where waters do swish,\n",
            "Swam a cod with a shimmering wish.\n",
            "He thought he could fly,\n",
            "Leap and touch the sky,\n",
            "But landed with a slippery splish.\n"
          ]
        }
      ],
      "source": [
        "with handler(provider):\n",
        "    print(limerick(\"fish\"))\n",
        "    print(\"-\" * 40)\n",
        "    print(limerick(\"fish\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e59acbc",
      "metadata": {},
      "source": [
        "If we want deterministic behavior, we can cache the template call. We can either cache it with the default `@functools.cache` or use LiteLLM's built-in cache by setting a cache backend and passing `caching=True` to the provider:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "706ce53b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Silent river glides,  \n",
            "Silver scales beneath the light,  \n",
            "Whispers from the deep.\n",
            "----------------------------------------\n",
            "Silent river glides,  \n",
            "Silver scales beneath the light,  \n",
            "Whispers from the deep.\n",
            "\n",
            "Silent waters dance,\n",
            "Colorful koi swim below,\n",
            "Nature's gentle art.\n",
            "----------------------------------------\n",
            "In water they glide,  \n",
            "Scales shimmering like jewels,  \n",
            "Silent waves they ride.\n",
            "\n",
            "Silvery scales gleam,\n",
            "In still waters, deep they glideâ€” \n",
            "Fish dance in the stream.\n",
            "----------------------------------------\n",
            "Silent waters flow,\n",
            "Colorful fish swim below,\n",
            "Nature's gentle glow.\n"
          ]
        }
      ],
      "source": [
        "@functools.cache\n",
        "@Template.define\n",
        "def haiku(theme: str) -> str:\n",
        "    \"\"\"Write a haiku on the theme of {theme}. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def haiku_no_cache(theme: str) -> str:\n",
        "    \"\"\"Write a haiku on the theme of {theme}. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "print()\n",
        "with handler(provider):\n",
        "    print(haiku(\"fish\"))\n",
        "    print(\"-\" * 40)\n",
        "    print(haiku(\"fish\"))\n",
        "\n",
        "print()\n",
        "# Enable LiteLLM caching by setting a cache backend and enabling caching.\n",
        "litellm.cache = Cache()\n",
        "provider_cached = LiteLLMProvider(caching=True)\n",
        "try:\n",
        "    with handler(provider_cached):\n",
        "        print(haiku_no_cache(\"fish2\"))\n",
        "        print(\"-\" * 40)\n",
        "        print(haiku_no_cache(\"fish2\"))\n",
        "finally:\n",
        "    litellm.cache = None\n",
        "\n",
        "print()\n",
        "with handler(provider):\n",
        "    print(haiku_no_cache(\"fish3\"))\n",
        "    print(\"-\" * 40)\n",
        "    print(haiku_no_cache(\"fish3\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13adb300",
      "metadata": {},
      "source": [
        "## Converting LLM Results to Python Objects\n",
        "\n",
        "Type conversion is handled by `decode`. By default, primitive types are converted. `DecodeError` is raised if a response cannot be converted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2c766859",
      "metadata": {},
      "outputs": [],
      "source": [
        "@Template.define\n",
        "def primes(first_digit: int) -> int:\n",
        "    \"\"\"Give a prime number with {first_digit} as the first digit. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "with handler(provider):\n",
        "    assert type(primes(6)) is int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d78a71",
      "metadata": {},
      "source": [
        "More complex types can be converted by providing handlers for `decode`. Callable synthesis is supported via `Encodable` and the evaluation providers in `effectful.handlers.llm.evaluation` (`UnsafeEvalProvider` or `RestrictedEvalProvider`), which enable parsing/compiling/executing synthesized code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c83bbdc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def count_a(input_string: str) -> int:\n",
            "    \"\"\"\n",
            "    Count the occurrences of the character 'a' in the given string.\n",
            "\n",
            "    Args:\n",
            "    input_string (str): The string to be searched.\n",
            "\n",
            "    Returns:\n",
            "    int: The number of times 'a' appears in the string.\n",
            "    \"\"\"\n",
            "    count: int = 0\n",
            "    for char in input_string:\n",
            "        if char == 'a':\n",
            "            count += 1\n",
            "    return count\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from collections.abc import Callable\n",
        "\n",
        "from effectful.handlers.llm.evaluation import UnsafeEvalProvider\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def count_char(char: str) -> Callable[[str], int]:\n",
        "    \"\"\"Write a function which takes a string and counts the occurrances of '{char}'. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "# Use UnsafeEvalProvider for simple examples; RestrictedEvalProvider may need extra globals.\n",
        "with handler(provider), handler(UnsafeEvalProvider()):\n",
        "    count_a = count_char(\"a\")\n",
        "    assert callable(count_a)\n",
        "    assert count_a(\"banana\") == 3\n",
        "    assert count_a(\"cherry\") == 0\n",
        "    # Print the source code of the generated function\n",
        "    print(inspect.getsource(count_a))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991ee445",
      "metadata": {},
      "source": [
        "## Tool Calling\n",
        "\n",
        "`Operation`s defined in the lexical scope of a `Template` are automatically available for the LLM to call as tools. The description of these operations is inferred from their type annotations and docstrings.\n",
        "\n",
        "Tool calls are mediated by a helper operation `tool_call`. Handling this operation allows tool use to be tracked or logged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "66711301",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool call: cities(*(), **{}) -> ['Chicago', 'New York', 'Barcelona']\n",
            "Tool call: weather(*('Chicago',), **{}) -> cold\n",
            "Tool call: weather(*('New York',), **{}) -> wet\n",
            "Tool call: weather(*('Barcelona',), **{}) -> sunny\n",
            "Based on the current weather descriptions:\n",
            "\n",
            "- **Chicago**: Cold\n",
            "- **New York**: Wet\n",
            "- **Barcelona**: Sunny\n",
            "\n",
            "Barcelona has sunny weather, which is often considered good weather by many people, so it would be a great choice!\n"
          ]
        }
      ],
      "source": [
        "@Tool.define\n",
        "def cities() -> list[str]:\n",
        "    \"\"\"Return a list of cities that can be passed to `weather`.\"\"\"\n",
        "    return [\"Chicago\", \"New York\", \"Barcelona\"]\n",
        "\n",
        "\n",
        "@Tool.define\n",
        "def weather(city: str) -> str:\n",
        "    \"\"\"Given a city name, return a description of the weather in that city.\"\"\"\n",
        "    status = {\"Chicago\": \"cold\", \"New York\": \"wet\", \"Barcelona\": \"sunny\"}\n",
        "    return status.get(city, \"unknown\")\n",
        "\n",
        "\n",
        "@Template.define  # cities and weather auto-captured from lexical scope\n",
        "def vacation() -> str:\n",
        "    \"\"\"Use the provided tools to suggest a city that has good weather. Use only the `cities` and `weather` tools provided.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "def log_tool_call(tool, *args, **kwargs):\n",
        "    result = fwd()\n",
        "    print(f\"Tool call: {tool}(*{args}, **{kwargs}) -> {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "with handler(provider), handler({Tool.__apply__: log_tool_call}):\n",
        "    print(vacation())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d221feb",
      "metadata": {},
      "source": [
        "## Structured Output Generation\n",
        "\n",
        "Constrained generation is used for any type that is convertible to a Pydantic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "17668ac8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> You are onstage at a comedy club. You tell the following joke:\n",
            "Knock knock.\n",
            "Who's there?\n",
            "Iguana.\n",
            "Iguana who?\n",
            "Iguana come in, it's really hot out here!\n",
            "> The crowd laughs politely.\n"
          ]
        }
      ],
      "source": [
        "@dataclasses.dataclass\n",
        "class KnockKnockJoke:\n",
        "    whos_there: str\n",
        "    punchline: str\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def write_joke(theme: str) -> KnockKnockJoke:\n",
        "    \"\"\"Write a knock-knock joke on the theme of {theme}. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def rate_joke(joke: KnockKnockJoke) -> bool:\n",
        "    \"\"\"Decide if {joke} is funny or not. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "def do_comedy():\n",
        "    joke = write_joke(\"lizards\")\n",
        "    print(\"> You are onstage at a comedy club. You tell the following joke:\")\n",
        "    print(\n",
        "        f\"Knock knock.\\nWho's there?\\n{joke.whos_there}.\\n{joke.whos_there} who?\\n{joke.punchline}\"\n",
        "    )\n",
        "    if rate_joke(joke):\n",
        "        print(\"> The crowd laughs politely.\")\n",
        "    else:\n",
        "        print(\"> The crowd stares in stony silence.\")\n",
        "\n",
        "\n",
        "with handler(provider):\n",
        "    do_comedy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cab62b5",
      "metadata": {},
      "source": [
        "### Logging LLM requests\n",
        "To intercept messages being called on the lower-level, we can write a handler for `completion`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cbf495a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Write a haiku on the theme of fish2. Do not use any tools.\n",
            "> {\"value\":\"Silver scales that gleam,\\nGentle ripples kiss the streamâ€”\\nFish in liquid dream.\"}\n",
            "{\"value\":\"Silver scales that gleam,\\nGentle ripples kiss the streamâ€”\\nFish in liquid dream.\"}\n",
            "> Write a limerick on the theme of fish. Do not use any tools.\n",
            "> {\"value\":\"There once was a fish quite spry,\\nWho leapt out to touch the sky,\\nBut with a splash and a grin,\\nBack to the waves with a spin,\\nIt swam off with hopes ever high.\"}\n",
            "{\"value\":\"There once was a fish quite spry,\\nWho leapt out to touch the sky,\\nBut with a splash and a grin,\\nBack to the waves with a spin,\\nIt swam off with hopes ever high.\"}\n"
          ]
        }
      ],
      "source": [
        "from effectful.handlers.llm.completions import get_message_sequence\n",
        "\n",
        "\n",
        "def log_llm(*args, **kwargs):\n",
        "    result = fwd(*args, **kwargs)\n",
        "\n",
        "    # Print all messages (to show injected instructions like error feedback)\n",
        "    if args and isinstance(args[0], list):\n",
        "        messages = args[0]\n",
        "    else:\n",
        "        messages = list(get_message_sequence().values())\n",
        "\n",
        "    for msg in messages:\n",
        "        content = msg.get(\"content\", \"\")\n",
        "        if isinstance(content, list):\n",
        "            text = \"\\n\".join(\n",
        "                block[\"text\"]\n",
        "                for block in content\n",
        "                if isinstance(block, dict) and block.get(\"type\") == \"text\"\n",
        "            )\n",
        "        else:\n",
        "            text = str(content)\n",
        "        print(f\"> {text}\")\n",
        "\n",
        "    message = result[0] if isinstance(result, tuple) else result\n",
        "    final_content = message.get(\"content\") or message.get(\"reasoning_content\")\n",
        "    if final_content is not None:\n",
        "        print(final_content)\n",
        "    return result\n",
        "\n",
        "\n",
        "# Avoid cache\n",
        "try:\n",
        "    haiku.cache_clear()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Put completion handler innermost so it has highest precedence during the call\n",
        "with handler(provider), handler({call_assistant: log_llm}):\n",
        "    _ = haiku(\"fish2\")\n",
        "    _ = limerick(\"fish\")  # or use haiku(\"fish-2\") to avoid cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0003944",
      "metadata": {},
      "source": [
        "## Template Composition\n",
        "\n",
        "Templates defined in the lexical scope are also captured, enabling template composition. One template can use the result of another template in a pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "78a4bf44",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sub-templates available to write_story: dict_keys(['limerick', 'haiku_no_cache', 'primes', 'count_char', 'cities', 'weather', 'vacation', 'write_joke', 'rate_joke', 'story_with_moral', 'story_funny', 'unstable_service', 'fetch_data', 'give_rating_for_movie'])\n",
            "=== Story with moral ===\n",
            "The curious cat, Misty, learned that while shiny objects may be appealing, the true value often lies in the adventures and memories created along the way. It's a lesson in appreciating the intangible riches of life.ðŸŒŸ\n",
            "\n",
            "=== Funny story ===\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Here's a funny story about Mr. Whiskers, the ever-curious cat, that led to laughter, mystery, and magical adventures for his entire village!\n"
          ]
        }
      ],
      "source": [
        "# Sub-templates for different story styles\n",
        "@Template.define\n",
        "def story_with_moral(topic: str) -> str:\n",
        "    \"\"\"Write a short story about {topic} and end with a moral lesson. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def story_funny(topic: str) -> str:\n",
        "    \"\"\"Write a funny, humorous story about {topic}. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "# Main orchestrator template - has access to sub-templates\n",
        "@Template.define\n",
        "def write_story(topic: str, style: str) -> str:\n",
        "    \"\"\"Write a story about {topic} in the style: {style}.\n",
        "    Available styles: 'moral' for a story with a lesson, 'funny' for humor. Use story_funny for humor, story_with_moral for a story with a lesson.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "\n",
        "# Verify sub-templates are captured in write_story's lexical context\n",
        "assert story_with_moral in write_story.tools.values()\n",
        "assert story_funny in write_story.tools.values()\n",
        "print(\"Sub-templates available to write_story:\", write_story.tools.keys())\n",
        "\n",
        "with handler(provider):\n",
        "    print(\"=== Story with moral ===\")\n",
        "    print(write_story(\"a curious cat\", \"moral\"))\n",
        "    print()\n",
        "    print(\"=== Funny story ===\")\n",
        "    print(write_story(\"a curious cat\", \"funny\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd25826d",
      "metadata": {},
      "source": [
        "### Retrying LLM Requests\n",
        "LLM calls can sometimes fail due to transient errors or produce invalid outputs. The `RetryLLMHandler` automatically retries failed template calls and can also surface tool/runtime errors as tool messages:\n",
        "\n",
        "- `num_retries`: Maximum number of retry attempts (default: 3)\n",
        "- `include_traceback`: When `True`, include traceback details in the error feedback (default: False)\n",
        "- `catch_tool_errors`: Exception type(s) to catch during tool execution (default: `Exception`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafc0a96",
      "metadata": {},
      "source": [
        "Example usage: having an unstable service that seldomly fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4334d07a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Service unavailable! Attempt 1/3. Please retry.\n",
            "Result: The data fetched from the unstable service is: [1, 2, 3]. Retries: 3\n"
          ]
        }
      ],
      "source": [
        "call_count = 0\n",
        "REQUIRED_RETRIES = 3\n",
        "\n",
        "\n",
        "@Tool.define\n",
        "def unstable_service() -> str:\n",
        "    \"\"\"Fetch data from an unstable external service. May require retries.\"\"\"\n",
        "    global call_count\n",
        "    call_count += 1\n",
        "    if call_count < REQUIRED_RETRIES:\n",
        "        raise ConnectionError(\n",
        "            f\"Service unavailable! Attempt {call_count}/{REQUIRED_RETRIES}. Please retry.\"\n",
        "        )\n",
        "    return \"{ 'status': 'ok', 'data': [1, 2, 3] }\"\n",
        "\n",
        "\n",
        "@Template.define  # unstable_service auto-captured from lexical scope\n",
        "def fetch_data() -> str:\n",
        "    \"\"\"Use the unstable_service tool to fetch data.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "with handler(provider):\n",
        "    try:\n",
        "        result = fetch_data()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "with handler(provider), handler(RetryLLMHandler(num_retries=3)):\n",
        "    result = fetch_data()\n",
        "    print(f\"Result: {result}\", \"Retries:\", call_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac00e01",
      "metadata": {},
      "source": [
        "### Retrying with Validation Errors\n",
        "As noted above, the `RetryHandler` can also be used to retry on runtime/validation error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b2b225",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 5/5\n",
            "Explanation: Die Hard deserves a 5 out of 5. This film is a quintessential action movie, showcasing Bruce Willis in an iconic role that defined his career. Its gripping plot, high-stakes action scenes, and outstanding villain performance by Alan Rickman make it a timeless classic. These elements together ensure its place as a peak action movie experience, hence the perfect score.\n"
          ]
        }
      ],
      "source": [
        "@pydantic.dataclasses.dataclass\n",
        "class Rating:\n",
        "    score: int\n",
        "    explanation: str\n",
        "\n",
        "    @field_validator(\"score\")\n",
        "    @classmethod\n",
        "    def check_score(cls, v):\n",
        "        if v < 1 or v > 5:\n",
        "            raise PydanticCustomError(\n",
        "                \"invalid_score\",\n",
        "                \"score must be 1â€“5, got {v}\",\n",
        "                {\"v\": v},\n",
        "            )\n",
        "        return v\n",
        "\n",
        "    @field_validator(\"explanation\")\n",
        "    @classmethod\n",
        "    def check_explanation_contains_score(cls, v, info):\n",
        "        score = info.data.get(\"score\", None)\n",
        "        if score is not None and str(score) not in v:\n",
        "            raise PydanticCustomError(\n",
        "                \"invalid_explanation\",\n",
        "                \"explanation must mention the score {score}, got '{explanation}'\",\n",
        "                {\"score\": score, \"explanation\": v},\n",
        "            )\n",
        "        return v\n",
        "\n",
        "\n",
        "@Template.define\n",
        "def give_rating_for_movie(movie_name: str) -> Rating:\n",
        "    \"\"\"Give a rating for {movie_name}. The explanation MUST include the numeric score. Do not use any tools.\"\"\"\n",
        "    raise NotHandled\n",
        "\n",
        "with handler(provider):\n",
        "    try:\n",
        "        rating = give_rating_for_movie(\"Die Hard\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "with handler(provider), handler(RetryLLMHandler(num_retries=3)):\n",
        "    rating = give_rating_for_movie(\"Die Hard\")\n",
        "    print(f\"Score: {rating.score}/5\")\n",
        "    print(f\"Explanation: {rating.explanation}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
