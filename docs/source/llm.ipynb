{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aaf649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import inspect\n",
    "import logging\n",
    "import sys\n",
    "from collections.abc import Callable\n",
    "\n",
    "from effectful.handlers.llm import Template, Tool\n",
    "from effectful.handlers.llm.providers import (\n",
    "    CacheLLMRequestHandler,\n",
    "    LiteLLMProvider,\n",
    "    LLMLoggingHandler,\n",
    "    RetryLLMHandler,\n",
    "    completion,\n",
    "    tool_call,\n",
    ")\n",
    "from effectful.handlers.llm.synthesis import ProgramSynthesis\n",
    "from effectful.ops.semantics import fwd, handler\n",
    "\n",
    "provider = LiteLLMProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e861b",
   "metadata": {},
   "source": [
    "## Interface\n",
    "\n",
    "The `robotl.ops.llm` module provides a simplified LLM interface that uses algebraic effects to provide modularity. The module interface consists of:\n",
    "\n",
    "- A decorator `template` which creates a prompt template from a callable. We should think of the prompt template as an LLM-implemented function with behavior specified by a template string. When a templated function is called, an LLM is invoked to produce the specified behavior. The `__call__` method of a template is a handleable operation.\n",
    "- An operation `decode` which parses LLM output. `decode(t: type, c: str)` converts an LLM response `c` to the type `t`. It can be handled to provide decoding logic for particular types.\n",
    "- Interpretations for LLM providers `OpenAIIntp` and callable decoding `ProgramSynthesisIntp`. These interpretations can be composed to handle a variety of template behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c639d3",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "This template function writes (bad) poetry on a given theme. While difficult to implement in Python, an LLM can provide a reasonable implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e832675",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def limerick(theme: str) -> str:\n",
    "    \"\"\"Write a limerick on the theme of {theme}.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca6919",
   "metadata": {},
   "source": [
    "If we call the template with a provider interpretation installed, we get reasonable behavior. The LLM is nondeterministic by default, so calling the template twice with the same arguments gives us different results.\n",
    "\n",
    "Templates are regular callables, so can be converted to operations with `defop` if we want to override the LLM implementation in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634f6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the ocean where fast fish dash,  \n",
      "Swims a mackerel that makes quite a splash.  \n",
      "With each flip of its fin,  \n",
      "It wears a cheeky grin,  \n",
      "And escapes from each net's eager clasp!  \n",
      "----------------------------------------\n",
      "In the depths of the ocean so blue,  \n",
      "Swam a fish with a curious view.  \n",
      "With a flick of its tail,  \n",
      "It set off to unveil,  \n",
      "The mysteries of waters anew.  \n"
     ]
    }
   ],
   "source": [
    "with handler(provider):\n",
    "    print(limerick(\"fish\"))\n",
    "    print(\"-\" * 40)\n",
    "    print(limerick(\"fish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59acbc",
   "metadata": {},
   "source": [
    "If we want deterministic behavior, we can cache the template call. We can either cache it with the default `@functools.cache` or using `CacheLLMRequestHandler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706ce53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Silver scales shimmer,  \n",
      "Dancing through the ocean's depths—  \n",
      "Whispers of the sea.\n",
      "----------------------------------------\n",
      "Silver scales shimmer,  \n",
      "Dancing through the ocean's depths—  \n",
      "Whispers of the sea.\n",
      "\n",
      "Silver scales shimmer,  \n",
      "Beneath the gentle waves' dance,  \n",
      "In the ocean's hush.  \n",
      "----------------------------------------\n",
      "Silver scales shimmer,  \n",
      "Beneath the gentle waves' dance,  \n",
      "In the ocean's hush.  \n",
      "\n",
      "In deep waters blue,  \n",
      "Silent dances shift and gleam,  \n",
      "Fish weave dreams anew.  \n",
      "----------------------------------------\n",
      "In deep waters blue,  \n",
      "Silent dances shift and gleam,  \n",
      "Fish weave dreams anew.  \n"
     ]
    }
   ],
   "source": [
    "@functools.cache\n",
    "@Template.define\n",
    "def haiku(theme: str) -> str:\n",
    "    \"\"\"Write a haiku on the theme of {theme}.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "@Template.define\n",
    "def haiku_no_cache(theme: str) -> str:\n",
    "    \"\"\"Write a haiku on the theme of {theme}.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print()\n",
    "with handler(provider):\n",
    "    print(haiku(\"fish\"))\n",
    "    print(\"-\" * 40)\n",
    "    print(haiku(\"fish\"))\n",
    "\n",
    "print()\n",
    "cache_handler1 = CacheLLMRequestHandler()\n",
    "with handler(provider), handler(cache_handler1):\n",
    "    print(haiku_no_cache(\"fish2\"))\n",
    "    print(\"-\" * 40)\n",
    "    print(haiku_no_cache(\"fish2\"))\n",
    "\n",
    "print()\n",
    "cache_handler2 = CacheLLMRequestHandler()\n",
    "with handler(provider), handler(cache_handler2):\n",
    "    print(haiku_no_cache(\"fish3\"))\n",
    "    print(\"-\" * 40)\n",
    "    print(haiku_no_cache(\"fish3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adb300",
   "metadata": {},
   "source": [
    "## Converting LLM Results to Python Objects\n",
    "\n",
    "Type conversion is handled by `decode`. By default, primitive types are converted. `DecodeError` is raised if a response cannot be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c766859",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Template.define\n",
    "def primes(first_digit: int) -> int:\n",
    "    \"\"\"Give a prime number with {first_digit} as the first digit.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "with handler(provider):\n",
    "    assert type(primes(6)) is int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d78a71",
   "metadata": {},
   "source": [
    "More complex types can be converted by providing handlers for `decode`. `ProgramSynthesisIntp` provides a `decode` handler that parses Python callables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c83bbdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def count_a_occurrences(input_string: str) -> int:\n",
      "    return input_string.count('a')\n"
     ]
    }
   ],
   "source": [
    "@Template.define\n",
    "def count_char(char: str) -> Callable[[str], int]:\n",
    "    \"\"\"Write a function which takes a string and counts the occurrances of '{char}'.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "with handler(provider), handler(ProgramSynthesis()):\n",
    "    count_a = count_char(\"a\")\n",
    "    assert callable(count_a)\n",
    "    assert count_a(\"banana\") == 3\n",
    "    assert count_a(\"cherry\") == 0\n",
    "    # Print the source code of the generated function\n",
    "    print(inspect.getsource(count_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ee445",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "Passing `Operation`s to `Template.define` makes them available for the LLM to call as tools. The description of these operations is inferred from their type annotations and docstrings.\n",
    "\n",
    "Tool calls are mediated by a helper operation `tool_call`. Handling this operation allows tool use to be tracked or logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66711301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: cities(*(), **{}) -> ['Chicago', 'New York', 'Barcelona']\n",
      "Tool call: weather(*(), **{'city': 'Chicago'}) -> cold\n",
      "Tool call: weather(*(), **{'city': 'New York'}) -> wet\n",
      "Tool call: weather(*(), **{'city': 'Barcelona'}) -> sunny\n",
      "Barcelona currently has good weather, as it is sunny.\n"
     ]
    }
   ],
   "source": [
    "@Tool.define\n",
    "def cities() -> list[str]:\n",
    "    return [\"Chicago\", \"New York\", \"Barcelona\"]\n",
    "\n",
    "\n",
    "@Tool.define\n",
    "def weather(city: str) -> str:\n",
    "    status = {\"Chicago\": \"cold\", \"New York\": \"wet\", \"Barcelona\": \"sunny\"}\n",
    "    return status.get(city, \"unknown\")\n",
    "\n",
    "\n",
    "@Template.define(tools=[cities, weather])\n",
    "def vacation() -> str:\n",
    "    \"\"\"Use the provided tools to suggest a city that has good weather.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def log_tool_call(_, tool, *args, **kwargs):\n",
    "    result = fwd()\n",
    "    print(f\"Tool call: {tool}(*{args}, **{kwargs}) -> {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "with handler(provider), handler({tool_call: log_tool_call}):\n",
    "    print(vacation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d221feb",
   "metadata": {},
   "source": [
    "## Structured Output Generation\n",
    "\n",
    "Constrained generation is used for any type that is convertible to a Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17668ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> You are onstage at a comedy club. You tell the following joke:\n",
      "Knock knock.\n",
      "Who's there?\n",
      "Iguana.\n",
      "Iguana who?\n",
      "Iguana come inside your house and warm up, it's cold out here!\n",
      "> The crowd laughs politely.\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass\n",
    "class KnockKnockJoke:\n",
    "    whos_there: str\n",
    "    punchline: str\n",
    "\n",
    "\n",
    "@Template.define\n",
    "def write_joke(theme: str) -> KnockKnockJoke:\n",
    "    \"\"\"Write a knock-knock joke on the theme of {theme}.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "@Template.define\n",
    "def rate_joke(joke: KnockKnockJoke) -> bool:\n",
    "    \"\"\"Decide if {joke} is funny or not\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def do_comedy():\n",
    "    joke = write_joke(\"lizards\")\n",
    "    print(\"> You are onstage at a comedy club. You tell the following joke:\")\n",
    "    print(\n",
    "        f\"Knock knock.\\nWho's there?\\n{joke.whos_there}.\\n{joke.whos_there} who?\\n{joke.punchline}\"\n",
    "    )\n",
    "    if rate_joke(joke):\n",
    "        print(\"> The crowd laughs politely.\")\n",
    "    else:\n",
    "        print(\"> The crowd stares in stony silence.\")\n",
    "\n",
    "\n",
    "with handler(provider):\n",
    "    do_comedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab62b5",
   "metadata": {},
   "source": [
    "### Logging LLM requests\n",
    "To intercept messages being called on the lower-level, we can write a handler for `completion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf495a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request fired:  () {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Write a haiku on the theme of fish2.'}], 'role': 'user'}], 'response_format': None, 'tools': []} ModelResponse(id='chatcmpl-CkjWzRIrVVqCuOSeRSdCW1nYZ2SG7', created=1765254145, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Gently in the stream,  \\nSilver scales in dappled light,  \\nSilent swirls below.  ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=23, prompt_tokens=34, total_tokens=57, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "Request fired:  () {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Write a limerick on the theme of fish.'}], 'role': 'user'}], 'response_format': None, 'tools': []} ModelResponse(id='chatcmpl-CkjX0o5CHnG7qL9LJT0PvvofD2OzU', created=1765254146, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='In the sea where the swift currents swish,  \\nLived a cod with an unyielding wish.  \\nHe dreamt of the sky,  \\nWhere seagulls would fly,  \\nBut alas, he remained just a fish.  ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=50, prompt_tokens=34, total_tokens=84, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "def log_llm(*args, **kwargs):\n",
    "    result = fwd()\n",
    "    print(\"Request fired: \", args, kwargs, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Avoid cache\n",
    "try:\n",
    "    haiku.cache_clear()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Put completion handler innermost so it has highest precedence during the call\n",
    "with handler(provider), handler({completion: log_llm}):\n",
    "    _ = haiku(\"fish2\")\n",
    "    _ = limerick(\"fish\")  # or use haiku(\"fish-2\") to avoid cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e531d",
   "metadata": {},
   "source": [
    "### Python logging for LLM requests and tool calls\n",
    "We can also uses Python logger through `LLMLoggingHandler` to log both low-level LLM requests (`completion`) and model-initiated tool use (`tool_call`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a15f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Write a haiku on the theme of fish3.'}], 'role': 'user'}], 'response_format': None, 'tools': []}, 'response': ModelResponse(id='chatcmpl-CkjX1vTtAKxu7ldqTHNLf3Q5HJtEa', created=1765254147, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Beneath ripples glide,  \\nWhispers of scales in moonlight,  \\nSilent depths, fish dart.  ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=25, prompt_tokens=34, total_tokens=59, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Write a limerick on the theme of fish4.'}], 'role': 'user'}], 'response_format': None, 'tools': []}, 'response': ModelResponse(id='chatcmpl-CkjX2ED38u6C82SrNwcgpoBJ6rLtL', created=1765254148, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='In the ocean so deep and so swish,  \\nSwam a cod with a dream-like wish.  \\nIt leaped with a flop,  \\nOver waves it would hop,  \\nSaying, \"One day I\\'ll fly—oh, what bliss!\"  ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=53, prompt_tokens=35, total_tokens=88, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a logger\n",
    "logger = logging.getLogger(\"effectful.llm\")\n",
    "logger.setLevel(logging.INFO)\n",
    "log_handler = logging.StreamHandler(sys.stdout)\n",
    "log_handler.setFormatter(logging.Formatter(\"%(levelname)s %(payload)s\"))\n",
    "logger.addHandler(log_handler)\n",
    "# 2. Pass it to the handler\n",
    "llm_logger = LLMLoggingHandler(logger=logger)  # can also be LLMLoggingHandler()\n",
    "\n",
    "# Avoid cache for demonstration\n",
    "try:\n",
    "    haiku.cache_clear()\n",
    "    limerick.cache_clear()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with handler(provider), handler(llm_logger):\n",
    "    _ = haiku(\"fish3\")\n",
    "    _ = limerick(\"fish4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25826d",
   "metadata": {},
   "source": [
    "### Retrying LLM Requests\n",
    "LLM calls can sometimes fail due to transient errors or produce invalid outputs. The `RetryLLMHandler` automatically retries failed template calls:\n",
    "\n",
    "- `max_retries`: Maximum number of retry attempts (default: 3)\n",
    "- `add_error_feedback`: When `True`, appends the error message to the prompt on retry, helping the LLM correct its output.\n",
    "- `exception_cls`: RetryHandler will only attempt to try again when a specific type of `Exception` is thrown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc0a96",
   "metadata": {},
   "source": [
    "Example usage: having an unstable service that seldomly fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4334d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Use the unstable_service tool to fetch data.'}], 'role': 'user'}], 'response_format': None, 'tools': [{'type': 'function', 'function': {'name': 'unstable_service', 'description': 'Fetch data from an unstable external service. May require retries.', 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}]}, 'response': ModelResponse(id='chatcmpl-CkjX4Tdccqd0ljEj2AUMlASFo31Tp', created=1765254150, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{}', name='unstable_service'), id='call_6cYMZnIK0hrv3xTStyyWBLXR', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=11, prompt_tokens=52, total_tokens=63, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Use the unstable_service tool to fetch data.'}], 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'name': 'unstable_service', 'content': \"{'status': 'failure', 'exception': 'Service unavailable! Attempt 1/3. Please retry.'}\"}], 'response_format': None, 'tools': [{'type': 'function', 'function': {'name': 'unstable_service', 'description': 'Fetch data from an unstable external service. May require retries.', 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}]}, 'response': ModelResponse(id='chatcmpl-CkjX4gCmm8GEpTADaz6b3WJHXETYu', created=1765254150, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{}', name='unstable_service'), id='call_7Cz1w1toF0CccR8e5XUp0dIP', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=11, prompt_tokens=95, total_tokens=106, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Use the unstable_service tool to fetch data.'}], 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'name': 'unstable_service', 'content': \"{'status': 'failure', 'exception': 'Service unavailable! Attempt 1/3. Please retry.'}\"}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_7Cz1w1toF0CccR8e5XUp0dIP', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_7Cz1w1toF0CccR8e5XUp0dIP', 'name': 'unstable_service', 'content': \"{'status': 'failure', 'exception': 'Service unavailable! Attempt 2/3. Please retry.'}\"}], 'response_format': None, 'tools': [{'type': 'function', 'function': {'name': 'unstable_service', 'description': 'Fetch data from an unstable external service. May require retries.', 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}]}, 'response': ModelResponse(id='chatcmpl-CkjX57HW80BM2iCR3guWCc8e9etYS', created=1765254151, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{}', name='unstable_service'), id='call_KLr43KhV2A4GzUbQfbhD1iXz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=11, prompt_tokens=138, total_tokens=149, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "INFO {'tool': 'unstable_service', 'args': (), 'kwargs': {}}\n",
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Use the unstable_service tool to fetch data.'}], 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_6cYMZnIK0hrv3xTStyyWBLXR', 'name': 'unstable_service', 'content': \"{'status': 'failure', 'exception': 'Service unavailable! Attempt 1/3. Please retry.'}\"}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_7Cz1w1toF0CccR8e5XUp0dIP', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_7Cz1w1toF0CccR8e5XUp0dIP', 'name': 'unstable_service', 'content': \"{'status': 'failure', 'exception': 'Service unavailable! Attempt 2/3. Please retry.'}\"}, {'content': None, 'role': 'assistant', 'tool_calls': [{'function': {'arguments': '{}', 'name': 'unstable_service'}, 'id': 'call_KLr43KhV2A4GzUbQfbhD1iXz', 'type': 'function'}], 'function_call': None, 'provider_specific_fields': {'refusal': None}, 'annotations': []}, {'role': 'tool', 'tool_call_id': 'call_KLr43KhV2A4GzUbQfbhD1iXz', 'name': 'unstable_service', 'content': [{'type': 'text', 'text': \"{ 'status': 'ok', 'data': [1, 2, 3] }\"}]}], 'response_format': None, 'tools': [{'type': 'function', 'function': {'name': 'unstable_service', 'description': 'Fetch data from an unstable external service. May require retries.', 'parameters': {'additionalProperties': False, 'properties': {}, 'title': 'Params', 'type': 'object', 'required': []}, 'strict': True}}]}, 'response': ModelResponse(id='chatcmpl-CkjX5GhYtff6cMPBmDxTmDp8PVvEr', created=1765254151, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='I successfully fetched the data: \\\\([1, 2, 3]\\\\).', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=18, prompt_tokens=178, total_tokens=196, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "Result: I successfully fetched the data: \\([1, 2, 3]\\). Retries: 3\n"
     ]
    }
   ],
   "source": [
    "call_count = 0\n",
    "REQUIRED_RETRIES = 3\n",
    "\n",
    "@defop\n",
    "def unstable_service() -> str:\n",
    "    \"\"\"Fetch data from an unstable external service. May require retries.\"\"\"\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    if call_count < REQUIRED_RETRIES:\n",
    "        raise ConnectionError(\n",
    "            f\"Service unavailable! Attempt {call_count}/{REQUIRED_RETRIES}. Please retry.\"\n",
    "        )\n",
    "    return \"{ 'status': 'ok', 'data': [1, 2, 3] }\"\n",
    "\n",
    "\n",
    "@Template.define(tools=[unstable_service])\n",
    "def fetch_data() -> str:\n",
    "    \"\"\"Use the unstable_service tool to fetch data.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "retry_handler = RetryLLMHandler(max_retries=5, add_error_feedback=True)\n",
    "\n",
    "with handler(provider), handler(retry_handler), handler(llm_logger):\n",
    "    result = fetch_data()\n",
    "    print(f\"Result: {result}\", \"Retries:\", call_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac00e01",
   "metadata": {},
   "source": [
    "### Retrying with Validation Errors\n",
    "As noted above, the `RetryHandler` can also be used to retry on runtime/validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39b2b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Give a rating for Die Hard. The explanation MUST include the numeric score.'}], 'role': 'user'}], 'response_format': <class 'effectful.handlers.llm.providers.Response'>, 'tools': []}, 'response': ModelResponse(id='chatcmpl-CkjbhOTBz1G18GHnmdx0IcPaqOIiB', created=1765254437, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"value\":{\"score\":9,\"explanation\":\"Die Hard is often regarded as a quintessential action film, praised for its innovative story, memorable characters, and thrilling sequences. The film\\'s protagonist, John McClane, played by Bruce Willis, is celebrated for his relatable and everyman qualities, which set a new standard for action heroes. Additionally, Alan Rickman\\'s portrayal of the villain Hans Gruber is highly acclaimed for adding depth and sophistication to the antagonist role. The movie\\'s pace, witty dialogues, and suspenseful action have made it a beloved classic in the action genre. For these reasons, it deserves a high score of 9 out of 10.\"}}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=138, prompt_tokens=108, total_tokens=246, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "INFO {'args': (), 'kwargs': {'messages': [{'type': 'message', 'content': [{'type': 'text', 'text': 'Retry generating the following prompt: Give a rating for Die Hard. The explanation MUST include the numeric score.\\n\\nError from previous generation:\\n```\\nTraceback (most recent call last):\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/handlers/llm/providers.py\", line 321, in _retry_completion\\n    return fwd()\\n           ^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/ops/types.py\", line 433, in __call__\\n    return self_handler(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\\n    return func(*args, **kwds)\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/internals/runtime.py\", line 45, in _cont_wrapper\\n    return fn(*a, **k)\\n           ^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/internals/runtime.py\", line 56, in _cont_wrapper\\n    return fn(*a, **k)\\n           ^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/internals/runtime.py\", line 70, in bound_body\\n    return body(*a, **k)\\n           ^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/internals/runtime.py\", line 56, in _cont_wrapper\\n    return fn(*a, **k)\\n           ^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/handlers/llm/providers.py\", line 471, in _call\\n    return decode_response(template, resp)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/ops/types.py\", line 449, in __call__\\n    return class_apply(self, *args, **kwargs)  # type: ignore[return-value]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/ops/types.py\", line 474, in apply\\n    return op.__default_rule__(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/ops/types.py\", line 334, in __default_rule__\\n    return self.__default__(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/effectful/handlers/llm/providers.py\", line 426, in decode_response\\n    result = Result.model_validate_json(result_str)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/datnguyenthanh/Marc/effectful/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 766, in model_validate_json\\n    return cls.__pydantic_validator__.validate_json(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\npydantic_core._pydantic_core.ValidationError: 1 validation error for Response\\nvalue.score\\n  score must be 1–5, got 9 [type=invalid_score, input_value=9, input_type=int]\\n```'}], 'role': 'user'}], 'response_format': <class 'effectful.handlers.llm.providers.Response'>, 'tools': []}, 'response': ModelResponse(id='chatcmpl-CkjbkDlrwCHGvzQNh2wdT28L7j19N', created=1765254440, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_83554c687e', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"value\":{\"score\":5,\"explanation\":\"Die Hard is a highly acclaimed action film widely regarded as a classic in its genre. It combines thrilling action sequences with a charismatic performance by Bruce Willis as the lead character. The film\\'s clever plot, high stakes, and memorable antagonist make it a favorite among action movie enthusiasts. Critics and audiences alike often rate it at the top end of action cinema, earning it a score of 5 out of 5.\"}}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=97, prompt_tokens=843, total_tokens=940, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')}\n",
      "Score: 5/5\n",
      "Explanation: Die Hard is a highly acclaimed action film widely regarded as a classic in its genre. It combines thrilling action sequences with a charismatic performance by Bruce Willis as the lead character. The film's clever plot, high stakes, and memorable antagonist make it a favorite among action movie enthusiasts. Critics and audiences alike often rate it at the top end of action cinema, earning it a score of 5 out of 5.\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "from pydantic import ValidationError, field_validator\n",
    "from pydantic_core import PydanticCustomError\n",
    "\n",
    "\n",
    "@pydantic.dataclasses.dataclass\n",
    "class Rating:\n",
    "    score: int\n",
    "    explanation: str\n",
    "\n",
    "    @field_validator(\"score\")\n",
    "    @classmethod\n",
    "    def check_score(cls, v):\n",
    "        if v < 1 or v > 5:\n",
    "            raise PydanticCustomError(\n",
    "                \"invalid_score\",\n",
    "                \"score must be 1–5, got {v}\",\n",
    "                {\"v\": v},\n",
    "            )\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"explanation\")\n",
    "    @classmethod\n",
    "    def check_explanation_contains_score(cls, v, info):\n",
    "        score = info.data.get(\"score\", None)\n",
    "        if score is not None and str(score) not in v:\n",
    "            raise PydanticCustomError(\n",
    "                \"invalid_explanation\",\n",
    "                \"explanation must mention the score {score}, got '{explanation}'\",\n",
    "                {\"score\": score, \"explanation\": v},\n",
    "            )\n",
    "        return v\n",
    "\n",
    "\n",
    "@Template.define\n",
    "def give_rating_for_movie(movie_name: str) -> Rating:\n",
    "    \"\"\"Give a rating for {movie_name}. The explanation MUST include the numeric score.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# RetryLLMHandler with error feedback - the traceback helps LLM correct validation errors\n",
    "# Note: Pydantic wraps PydanticCustomError inside ValidationError, so we catch ValidationError instead\n",
    "retry_handler = RetryLLMHandler(\n",
    "    max_retries=3,\n",
    "    add_error_feedback=True,\n",
    "    exception_cls=ValidationError,  # Catch validation errors\n",
    ")\n",
    "\n",
    "with handler(provider), handler(retry_handler), handler(llm_logger):\n",
    "    rating = give_rating_for_movie(\"Die Hard\")\n",
    "    print(f\"Score: {rating.score}/5\")\n",
    "    print(f\"Explanation: {rating.explanation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effectful",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
